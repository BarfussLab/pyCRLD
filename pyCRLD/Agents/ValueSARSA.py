"""CRLD SARSA agents in value space"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/Agents/05_AValueSARSA.ipynb.

# %% auto 0
__all__ = ['valSARSA', 'RPEisa', 'valRisa', 'valNextQisa']

# %% ../../nbs/Agents/05_AValueSARSA.ipynb 18
import numpy as np
import itertools as it
from functools import partial

import jax
from jax import jit
import jax.numpy as jnp

from fastcore.utils import *

from .ValueBase import valuebase
from ..Utils.Helpers import *

# %% ../../nbs/Agents/05_AValueSARSA.ipynb 19
class valSARSA(valuebase):
    """
    Class for CRLD-SARSA agents in value space.
    """

# %% ../../nbs/Agents/05_AValueSARSA.ipynb 21
@partial(jit, static_argnums=(0,2))
def RPEisa(self:valSARSA,
           Qisa,  # Joint strategy
           norm=False # normalize error around actions? 
           ) -> np.ndarray:  # reward-prediction error
    """
    Compute temporal-difference reward-prediction error for 
    value SARSA dynamics, given joint state-action values `Qisa`.
    """
    Risa = self.valRisa(Qisa)
    NextQisa = self.value_NextQisa(Qisa)
    
    n = jnp.newaxis
    E = self.pre[:,n,n]*Risa + self.gamma[:,n,n]*NextQisa - Qisa
    
    E = E - E.mean(axis=2, keepdims=True) if norm else E
    return E
valSARSA.RPEisa = RPEisa

# %% ../../nbs/Agents/05_AValueSARSA.ipynb 22
@partial(jit, static_argnums=0)
def valRisa(self:valSARSA, 
            Qisa): # Joint state-action values
    """ Average reward Risa, given joint state-action values `Qisa` """
    Xisa = self.strategy_function.action_probabilities(Qisa)
    Risa = self.Risa(Xisa)
    return Risa
valSARSA.valRisa = valRisa

# %% ../../nbs/Agents/05_AValueSARSA.ipynb 23
@partial(jit, static_argnums=0)
def valNextQisa(self:valSARSA, 
                Qisa):   
    """
    Compute strategy-average next state-action value for agent `i`, current
    state `s` and action `a`, given joint state-action values `Qisa`.
    """
    Xisa = self.strategy_function.action_probabilities(Qisa)
    valQisa = self.Qisa(Xisa)  # true state-action values given current Qisa

    i = 0  # agent i
    a = 1  # its action a
    s = 2  # the current state
    sprim = 3  # the next state
    j2k = list(range(4, 4+self.N-1))  # other agents
    b2d = list(range(4+self.N-1, 4+self.N-1 + self.N))  # all actions
    e2f = list(range(3+2*self.N, 3+2*self.N + self.N-1))  # all other acts

    sumsis = [[j2k[l], s, e2f[l]] for l in range(self.N-1)]  # sum inds
    otherX = list(it.chain(*zip((self.N-1)*[Xisa], sumsis)))
        
    NextQisa = jnp.einsum(valQisa, [i, s, a], Xisa, [i, s, a], [i, s])
                
    args = [self.Omega, [i]+j2k+[a]+b2d+e2f] + otherX +\
        [self.T, [s]+b2d+[sprim], NextQisa, [i, sprim], [i, s, a]]
    return jnp.einsum(*args, optimize=self.opti)
valSARSA.value_NextQisa = valNextQisa  

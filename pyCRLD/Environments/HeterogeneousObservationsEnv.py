# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/Environments/02_HeterogeneousObservationsEnv.ipynb.

# %% auto 0
__all__ = ['HeterogeneousObservationsEnv']

# %% ../../nbs/Environments/02_HeterogeneousObservationsEnv.ipynb 2
from fastcore.utils import *
import numpy as np

# %% ../../nbs/Environments/02_HeterogeneousObservationsEnv.ipynb 4
class HeterogeneousObservationsEnv(object):
    def __init__(self,
         reward:float,  # reward of mutual cooperation
         temptation:float,  # temptation of unilateral defection 
         suckers_payoff:float,  # sucker's payoff of unilateral cooperation
         punishment:float,
         observation_confidence=None):

        self.reward = reward
        self.temptation = temptation
        self.suckers_payoff = suckers_payoff
        self.punishment = punishment

        self.transitions = self.transition_tensor()
        self.final_states = np.array(self.generate_final_states())
        self.rewards = self.reward_tensor()
        self.observations = self.generate_observation_tensors()

        self.actions_set = self.actions()
        self.states_set = self.states() 
        self.observations_set = self.generate_observation_labels()

        self.n_agents = self.rewards.shape[0]
        self.n_states = self.transitions.shape[0]
        self.n_agent_actions = self.transitions.shape[1]

        self.defaultObsTensUsed

        # By default, agents are fully observable
        if observation_confidence is None:
            self.observation_confidence = [1] * self.n_agents
        else:
            self.observation_confidence = observation_confidence
        assert self.observation_confidence == self.n_agents, 'Observation confidences need to be specified for all agents or for none and receive the full observability as a default value'

        # Checks
        # assert all(self.transitions.shape[1:-1] == self.n_agent_actions for _ in range(self.n_agents)), 'Inconsistent number of actions'
        assert all(dim == self.n_agent_actions for dim in self.rewards.shape[2:-1]), 'Inconsistent number of actions'
        assert len(self.actions_set) == self.n_agents and all(len(a) == self.n_agent_actions for a in self.actions_set), 'Inconsistent number of actions'
        assert self.transitions.shape[-1] == self.n_states and self.rewards.shape[-1] == self.n_states, 'Inconsistent number of states'
        assert self.rewards.shape[1] == self.n_states, 'Inconsistent number of states'
        assert len(self.final_states) == self.n_states, 'Inconsistent number of states'
        assert len(self.states_set) == self.n_states, 'Inconsistent number of states'
        assert np.allclose(self.transitions.sum(-1), 1), 'Transition model probabilities do not sum to 1'

        #     assert obs.shape[0] == self.n_agents, "Inconsistent number of agents"
        #     assert obs.shape[1] == self.n_states, "Inconsistent number of states"
        #     assert np.allclose(obs.sum(-1), 1), 'Observation model probabilities do not sum to 1'

# %% ../../nbs/Environments/02_HeterogeneousObservationsEnv.ipynb 5
@patch
def id(self:HeterogeneousObservationsEnv):
    """Returns id string of environment"""
    return f"{self.__class__.__name__}"

@patch
def __str__(self:HeterogeneousObservationsEnv): return self.id()

@patch
def __repr__(self:HeterogeneousObservationsEnv): return self.id()

@patch
def transition_tensor(self:HeterogeneousObservationsEnv):
    raise NotImplementedError

@patch
def reward_tensor(self:HeterogeneousObservationsEnv):
    raise NotImplementedError

@patch
def generate_observation_tensor(self:HeterogeneousObservationsEnv):
    """
    Generates the observation tensor for the environment. The tensor represents the probability distribution over possible
    observations given the actual state of the environment. The shape of the tensor depends on the observation confidence 
    of each agent.
    """
    if np.all(self.noise > 0.5):
        self.n_observations = 1
        observations_iso = np.ones((self.n_agents, self.n_states, self.n_observations))
        
    else:
        self.n_observations = self.n_states
        observations_iso = np.zeros((self.n_agents, self.n_states, self.n_observations))
    
        for i in range(self.n_agents):
            observations_iso[i,0,0] = 1 - min(self.observation_confidence[i], 0.5)
            observations_iso[i,0,1] = 0 + min(self.observation_confidence[i], 0.5)
            observations_iso[i,1,0] = 0 + min(self.observation_confidence[i], 0.5)
            observations_iso[i,1,1] = 1 - min(self.observation_confidence[i], 0.5)
        
    return observations_iso

@patch
def generate_final_states(self:HeterogeneousObservationsEnv):
    """Default final states: no final states"""
    return np.zeros(self.n_states, dtype=int)

@patch
def actions(self:HeterogeneousObservationsEnv):
    """Default action set representations."""
    return [[str(a) for a in range(self.n_agent_actions)] for _ in range(self.n_agents)]

@patch
def states(self:HeterogeneousObservationsEnv):
    """Default state set representation."""
    return [str(s) for s in range(self.n_states)]

@patch
def generate_observation_labels(self:HeterogeneousObservationsEnv):
    """Creates observation labels."""
    return [[str(o) for o in range(self.n_observations)] for _ in range(self.n_agents)]

@patch
def step(self:HeterogeneousObservationsEnv, jA:Iterable) -> tuple:
    """Iterate the environment one step forward."""
    tps = self.transitions[tuple([self.state]+list(jA))].astype(float)
    next_state = np.random.choice(range(len(tps)), p=tps)
    rewards = self.rewards[tuple([slice(self.n_agents), self.state]+list(jA)+[next_state])]
    self.state = next_state
    obs = self.generate_stochastic_observations()
    done = self.state in np.where(self.final_states==1)[0]
    info = {'state': self.state}
    return obs, rewards.astype(float), done, info

@patch
def generate_stochastic_observations(self:HeterogeneousObservationsEnv) -> np.ndarray:
    """
    Produces a set of observations for each agent based on the current state, utilizing the defined observation tensors.
    Each tensor represents a different observation model, and this method generates observations according to the probability
    distributions specified in those tensors for the current state.
    
    Returns:
        A list of numpy arrays, where each array contains observations for all agents as determined by one of the observation tensors.
    """
    all_agents_observations = []  # Stores observations generated by each observation tensor.
    for observation_tensor in self.observations_list:
        current_state_observations = np.zeros(self.n_agents, dtype=int)  # Initializes the observation array for this tensor.
        for agent_index in range(self.n_agents):
            # Retrieves the probability distribution of observations for the current agent and state from the tensor.
            observation_probabilities = observation_tensor[agent_index, self.state]
            # Generates a random observation based on the probability distribution.
            chosen_observation = np.random.choice(range(len(observation_probabilities)), p=observation_probabilities)
            current_state_observations[agent_index] = chosen_observation
        all_agents_observations.append(current_state_observations)
    return all_agents_observations

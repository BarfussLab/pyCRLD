# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/Environments/02_HeterogeneousObservationsEnv.ipynb.

# %% auto 0
__all__ = ['HeterogeneousObservationsEnv']

# %% ../../nbs/Environments/02_HeterogeneousObservationsEnv.ipynb 2
from fastcore.utils import *
import numpy as np

# %% ../../nbs/Environments/02_HeterogeneousObservationsEnv.ipynb 4
class HeterogeneousObservationsEnv(object):
    def __init__(self, observation_type='default', observation_value=None):

        # Observation configurations
        self.observation_type = observation_type
        self.observation_value = observation_value

        self.transitions = self.transition_tensor()
        self.final_states = np.array(self.generate_final_states())
        self.rewards = self.reward_tensor()
        self.observations = self.generate_observation_tensor()

        self.actions_set = self.actions()
        self.states_set = self.states() 
        self.observation_set = self.generate_observation_set()

        self.n_agents = self.rewards.shape[0]
        self.n_states = self.transitions.shape[0]
        self.n_agent_actions = self.transitions.shape[1]

        self.n_possible_observations = self.n_states

        # Checks
        # assert all(self.transitions.shape[1:-1] == self.n_agent_actions for _ in range(self.n_agents)), 'Inconsistent number of actions'
        assert all(dim == self.n_agent_actions for dim in self.rewards.shape[2:-1]), 'Inconsistent number of actions'
        assert len(self.actions_set) == self.n_agents and all(len(a) == self.n_agent_actions for a in self.actions_set), 'Inconsistent number of actions'
        assert self.transitions.shape[-1] == self.n_states and self.rewards.shape[-1] == self.n_states, 'Inconsistent number of states'
        assert self.rewards.shape[1] == self.n_states, 'Inconsistent number of states'
        assert len(self.final_states) == self.n_states, 'Inconsistent number of states'
        assert len(self.states_set) == self.n_states, 'Inconsistent number of states'
        assert np.allclose(self.transitions.sum(-1), 1), 'Transition model probabilities do not sum to 1'

        # Ensure naming compatibility with the rest of PyCRDT
        self.R = self.rewards
        self.N = self.n_agents
        self.F = self.final_states
        self.M = self.n_agent_actions
        self.Z = self.n_states
        self.T = self.transitions
        self.O = self.observations
        self.Oset = self.observation_set
        self.Q = self.n_possible_observations
        self.Aset = self.actions_set
        self.Sset = self.states_set
        
        #     assert obs.shape[0] == self.n_agents, "Inconsistent number of agents"
        #     assert obs.shape[1] == self.n_states, "Inconsistent number of states"
        #     assert np.allclose(obs.sum(-1), 1), 'Observation model probabilities do not sum to 1'

# %% ../../nbs/Environments/02_HeterogeneousObservationsEnv.ipynb 5
@patch
def id(self:HeterogeneousObservationsEnv):
    """Returns id string of environment"""
    return f"{self.__class__.__name__}"

@patch
def __str__(self:HeterogeneousObservationsEnv): return self.id()

@patch
def __repr__(self:HeterogeneousObservationsEnv): return self.id()

@patch
def transition_tensor(self:HeterogeneousObservationsEnv):
    raise NotImplementedError
@patch
def reward_tensor(self:HeterogeneousObservationsEnv):
    raise NotImplementedError

@patch
def generate_observation_tensor(self:HeterogeneousObservationsEnv):
    self.n_possible_observations = self.n_states
    # Initialize the observation tensor filled with zeros.
    observations_iso = np.zeros((self.n_agents, self.n_states, self.n_possible_observations))

    # Check if observation_type and observation_value are lists
    if isinstance(self.observation_type, list) and isinstance(self.observation_value, list):
        for agent_index, (obs_type, focused_value) in enumerate(zip(self.observation_type, self.observation_value)):
            self._apply_observation_configuration(agent_index, obs_type, focused_value, observations_iso)
    else:
        # Handle the case where observation_type and observation_value are single values
        obs_type = self.observation_type
        focused_value = self.observation_value if isinstance(self.observation_value, list) else [self.observation_value]
        for agent_index in range(self.n_agents):
            self._apply_observation_configuration(agent_index, obs_type, focused_value[0], observations_iso)

    return observations_iso

@patch
def _apply_observation_configuration(self:HeterogeneousObservationsEnv, agent_index, obs_type, focused_value, observations_iso):
    if obs_type == 'default':
        for state in range(self.n_states):
            if self.n_possible_observations > 1:
                remaining_value = (1 - focused_value) / (self.n_possible_observations - 1)
            else:
                remaining_value = 0.0
            observations_iso[agent_index, state, :] = remaining_value
            observations_iso[agent_index, state, state] = focused_value
    elif obs_type == 'diagonal_confidence':
        for state in range(self.n_states):
            observations_iso[agent_index, state, state] = focused_value
    elif obs_type == 'fill':
        observations_iso[agent_index, :, :] = focused_value

@patch
def generate_final_states(self:HeterogeneousObservationsEnv):
    """Default final states: no final states"""
    return np.zeros(self.n_states, dtype=int)

@patch
def actions(self:HeterogeneousObservationsEnv):
    """Default action set representations."""
    return [[str(a) for a in range(self.n_agent_actions)] for _ in range(self.n_agents)]

@patch
def states(self:HeterogeneousObservationsEnv):
    """Default state set representation."""
    return [str(s) for s in range(self.n_states)]

@patch
def generate_observation_set(self:HeterogeneousObservationsEnv):
    """Creates observation labels."""
    # Currently this only generates labels for games with 1 state: '.'
    return [[str('.') for o in range(self.n_possible_observations)] for _ in range(self.n_agents)]

@patch
def step(self:HeterogeneousObservationsEnv, jA:Iterable) -> tuple:
    """Iterate the environment one step forward."""
    tps = self.transitions[tuple([self.state]+list(jA))].astype(float)
    next_state = np.random.choice(range(len(tps)), p=tps)
    rewards = self.rewards[tuple([slice(self.n_agents), self.state]+list(jA)+[next_state])]
    self.state = next_state
    obs = self.generate_stochastic_observations()
    done = self.state in np.where(self.final_states==1)[0]
    info = {'state': self.state}
    return obs, rewards.astype(float), done, info

@patch
def generate_stochastic_observations(self:HeterogeneousObservationsEnv) -> np.ndarray:
    """
    Produces a set of observations for each agent based on the current state, utilizing the defined observation tensors.
    Each tensor represents a different observation model, and this method generates observations according to the probability
    distributions specified in those tensors for the current state.
    
    Returns:
        A list of numpy arrays, where each array contains observations for all agents as determined by one of the observation tensors.
    """
    # TODO: I need to make sure observations are being generated correctly here
    # TODO: I need to ensure the observation arrays sum up to 1

    all_agents_observations = []  # Stores observations generated by each observation tensor.
    for observation_tensor in self.observations_list:
        current_state_observations = np.zeros(self.n_agents, dtype=int)  # Initializes the observation array for this tensor.
        for agent_index in range(self.n_agents):
            # Retrieves the probability distribution of observations for the current agent and state from the tensor.
            observation_probabilities = observation_tensor[agent_index, self.state]
            # Generates a random observation based on the probability distribution.
            chosen_observation = np.random.choice(range(len(observation_probabilities)), p=observation_probabilities)
            current_state_observations[agent_index] = chosen_observation
        all_agents_observations.append(current_state_observations)
    return all_agents_observations

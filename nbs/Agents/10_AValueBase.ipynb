{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Base\n",
    "\n",
    "> Base class containing the core methods of CRLD agents in value space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Agents/ValueBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from functools import partial\n",
    "\n",
    "# import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "from typing import Iterable\n",
    "from fastcore.utils import *\n",
    "\n",
    "from pyCRLD.Agents.Base import abase\n",
    "from pyCRLD.Utils.Helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Functions\n",
    "First, we define classes for different stragegy functions which are necessary for value-based agents. Then, we define the base class for value-based agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class multiagent_epsilongreedy_strategy():\n",
    "    \"\"\"A multiagent epsilon-greedy strategy in tabular form\"\"\"\n",
    "        \n",
    "    def __init__(self, epsilon_greedys=None, N=None):\n",
    "        \"\"\"\n",
    "        Policy class to create a multiagent epsilon-greedy policy.\n",
    "        \n",
    "        epsilon_greedys : iterable or float\n",
    "            if iterable: contains exploration parameter for each agent or\n",
    "            if float: contains exploration parameter for all agents \n",
    "        N : int\n",
    "            number of agents, only allowed if `epsilon_greedys` is single float\n",
    "        \"\"\"\n",
    "        egiter = hasattr(epsilon_greedys, '__iter__')\n",
    "    \n",
    "        if egiter:  # eps greedy iter, sm not\n",
    "            self.N = len(epsilon_greedys) # Number of agents\n",
    "            assert N is None, \"'N' must not be specified when iterable is given\"\n",
    "            \n",
    "        else: \n",
    "            self.N = N  # Number of agents\n",
    "            assert epsilon_greedys is not None, \"epsilon value must be given\"\n",
    "            assert type(epsilon_greedys) is float, 'Confusing parameter input'\n",
    "            epsilon_greedys = [epsilon_greedys] * self.N\n",
    "            \n",
    "        # exploration values\n",
    "        self.epsilongreedy_explorations =\\\n",
    "            jnp.array(epsilon_greedys).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@partial(jit, static_argnums=0)\n",
    "def action_probabilities(self:multiagent_epsilongreedy_strategy, Qisa):\n",
    "    \"\"\"Transform Q values into epsilongreedy policy\"\"\"\n",
    "    n = jnp.newaxis\n",
    "    Xisa = jnp.zeros_like(Qisa)\n",
    "        \n",
    "    # where are the actions with maximal value?\n",
    "    WhereMAXisa = Qisa == jnp.max(Qisa, axis=-1, keepdims=True)\n",
    "        \n",
    "    # assign 1-eps probability to max actions\n",
    "    eps = self.epsilongreedy_explorations\n",
    "    Xisa += (1-eps[:,n,n]) * WhereMAXisa /\\\n",
    "        WhereMAXisa.sum(axis=-1, keepdims=True)\n",
    "        \n",
    "    # assign eps probability to all actions\n",
    "    Qisa_dimensions = jnp.ones_like(Qisa)\n",
    "    Xisa += eps[:,n,n] * Qisa_dimensions\\\n",
    "        / Qisa_dimensions.sum(axis=-1, keepdims=True)\n",
    "    return Xisa \n",
    "\n",
    "# Monkey-patching - possibly problematic, but allows seperating the function\n",
    "# definition from the class definition into different cells\n",
    "multiagent_epsilongreedy_strategy.action_probabilities = action_probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def id(self:multiagent_epsilongreedy_strategy\n",
    "       ) -> str: # id \n",
    "    \"\"\"Returns an identifier to handle simulation runs.\"\"\"\n",
    "    id = f\"j{self.__class__.__name__}_\"\n",
    "    for i in range(self.N):\n",
    "        eg = np.array(self.epsilongreedy_explorations[i])\n",
    "        id += jnp.array_str(eg, precision=5)+'_'        \n",
    "    \n",
    "    return id[:-1]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Base Class\n",
    "Now we define the base clase for the value-based CRLD agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class valuebase(abase):\n",
    "    \"\"\"\n",
    "    Base class for deterministic strategy-average independent (multi-agent) reward-prediction temporal-difference reinforcement learning in value space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 env, # An environment object\n",
    "                 learning_rates:Union[float, Iterable], # agents' learning rates\n",
    "                 discount_factors:Union[float, Iterable], # agents' discount factors\n",
    "                 strategy_function, # the strategy function object\n",
    "                 choice_intensities:Union[float, Iterable]=1.0, # agents' choice intensities\n",
    "                 use_prefactor=False,  # use the 1-DiscountFactor prefactor\n",
    "                 opteinsum=True,  # optimize einsum functions\n",
    "                 **kwargs):\n",
    "\n",
    "        self.env = env\n",
    "        Tt = env.T; assert np.allclose(Tt.sum(-1), 1)\n",
    "        Rt = env.R    \n",
    "        super().__init__(Tt, Rt, discount_factors, use_prefactor, opteinsum)\n",
    "        self.F = jnp.array(env.F)\n",
    "\n",
    "        # learning rates\n",
    "        self.alpha = make_variable_vector(learning_rates, self.N)\n",
    "\n",
    "        # strategy function\n",
    "        assert env.N == strategy_function.N,\\\n",
    "            'Environment and strategy function must have the same number of\\\n",
    "             agents `N`'\n",
    "        self.strategy_function = strategy_function\n",
    "\n",
    "        # temporal difference error mirror (without indices)\n",
    "        self.TDerror = self.RPEisa      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@partial(jit, static_argnums=0)\n",
    "def step(self:valuebase, \n",
    "         Qisa):  # joint state-action values\n",
    "    \"\"\"\n",
    "    Temporal-difference reward-prediction learning step in value space,\n",
    "    given joint state-action values `Qisa`.\n",
    "    \"\"\"\n",
    "    RPisa = self.TDerror(Qisa)\n",
    "    Qisa_ = Qisa + self.alpha * RPisa\n",
    "    return Qisa_, RPisa\n",
    "valuebase.step = step  # Monkey-patching - possibly problematic, but allows seperating the function definition from the class definition into different cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def zero_intelligence_values(self:valuebase,\n",
    "                             value:float=0.0): # state-action value\n",
    "    \"\"\"\n",
    "    Zero-intelligence causes a behavior where agents choose each action with\n",
    "    equal probability.\n",
    "    \n",
    "    This function returns the state-action values for the zero-intelligence\n",
    "    strategy with each state-action value set to `value`.\n",
    "    \"\"\"\n",
    "    return value * jnp.ones((self.N, self.Z, self.M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def random_values(self:valuebase):\n",
    "    \"\"\"Returns normally distributed random state-action values.\"\"\"\n",
    "    return jnp.array(np.random.randn(self.N, self.Z, self.M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def id(self:valuebase\n",
    "       ) -> str: # id \n",
    "    \"\"\"Returns an identifier to handle simulation runs.\"\"\"\n",
    "    envid = self.env.id() + \"__\"\n",
    "    agentsid = f\"j{self.__class__.__name__}_\"\\\n",
    "        + f\"{str(self.alpha)}_{str(self.gamma)}_pre{self.use_prefactor}__\"\n",
    "    strategyid = self.strategy_function.id()\n",
    "    \n",
    "    return envid + agentsid + agentsid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

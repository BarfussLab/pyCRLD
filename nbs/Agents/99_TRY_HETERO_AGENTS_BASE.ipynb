{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base\n",
    "\n",
    "> Base class containing the core methods of CRLD agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Agents/MultipleObservationsAgentHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from typing import Iterable\n",
    "from fastcore.utils import *\n",
    "\n",
    "from pyCRLD.Utils.Helpers import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The agent base class\n",
    "contains core methods to compute the strategy-average reward-prediction error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HeteroObsAgentBase(object):\n",
    "    \"\"\"\n",
    "    Base class for deterministic strategy-average independent (multi-agent)\n",
    "    temporal-difference reinforcement learning. This class provides foundational\n",
    "    functionality for multi-agent reinforcement learning (MARL) systems, handling\n",
    "    the computation of average transition models, rewards, and value functions\n",
    "    based on the collective strategies of all agents in the environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 TransitionTensor: np.ndarray,  # Transition model of the environment.\n",
    "                 RewardTensor: np.ndarray,      # Reward model of the environment.\n",
    "                 DiscountFactors: Iterable[float],  # Discount factors for each agent.\n",
    "                 ObservationTensors: List[np.ndarray],\n",
    "                 DiscountFactors: Iterable[float],\n",
    "                 use_prefactor=False,  # Whether to scale values by (1 - discount factor).\n",
    "                 opteinsum=True):      # Whether to optimize einsum computations.\n",
    "        \"\"\"\n",
    "        Initializes the base class for MARL systems with essential models and parameters.\n",
    "        \n",
    "        Args:\n",
    "            TransitionTensor (np.ndarray): A tensor representing the transition probabilities\n",
    "                between states in the environment, given the actions of all agents.\n",
    "            RewardTensor (np.ndarray): A tensor representing the rewards received by agents,\n",
    "                given the current state, the actions of all agents, and the next state.\n",
    "            DiscountFactors (Iterable[float]): A collection of discount factors for each agent,\n",
    "                indicating how future rewards are valued relative to immediate rewards.\n",
    "            ObservationTensors: List[np.ndarray],\n",
    "            DiscountFactors: Iterable[float],\n",
    "            use_prefactor (bool): If True, scales value function computations by (1 - discount factor)\n",
    "                to keep values on the same scale as immediate rewards.\n",
    "            opteinsum (bool): If True, enables optimization of einsum operations for efficiency.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize observation tensors per agent.\n",
    "        self.observations = [jnp.array(O_i) for O_i in ObservationTensors]\n",
    "        self.n_observations_per_agent = [O_i.shape[-1] for O_i in self.O]\n",
    "        \n",
    "        # Convert input tensors to JAX numpy arrays for efficiency in computation.\n",
    "        self.rewards = jnp.array(RewardTensor)\n",
    "        self.transitions = jnp.array(TransitionTensor)\n",
    "    \n",
    "        # Extract the number of agents from the first dimension of the reward tensor.\n",
    "        self.n_agents = self.rewards.shape[0]  \n",
    "        assert len(T.shape[1:-1]) == self.n_agents, \"Inconsistent number of agents with the transition tensor.\"\n",
    "        assert len(R.shape[2:-1]) == self.n_agents, \"Inconsistent number of agents with the reward tensor.\"\n",
    "        \n",
    "        # Determine the number of actions for each agent from the transition tensor.\n",
    "        self.n_agent_actions = self.transitions.shape[1] \n",
    "        assert np.allclose(self.transitions.shape[1:-1], n_agent_actions), 'Inconsistent number of actions across dimensions.'\n",
    "        assert np.allclose(self.rewards.shape[2:-1], n_agent_actions), 'Inconsistent number of actions with the reward tensor.'\n",
    "        \n",
    "        # Determine the number of states from the dimensions of the transition tensor.\n",
    "        self.n_states = self.transitions.shape[0] \n",
    "        assert self.transitions.shape[-1] == self.n_states, 'Inconsistent number of states in the transition tensor.'\n",
    "        assert self.rewards.shape[-1] == self.n_states, 'Inconsistent number of states in the reward tensor.'\n",
    "        assert self.rewards.shape[1] == self.n_states, 'Inconsistent number of states across dimensions of the reward tensor.'\n",
    "        \n",
    "        # Store class variables for use in later computations.\n",
    "        self.Q = self.n_states\n",
    "        \n",
    "        # Convert discount factors to a JAX numpy array and store.\n",
    "        self.gamma = make_variable_vector(DiscountFactors, N)\n",
    "\n",
    "        # Determine whether to scale value function computations by (1 - discount factor).\n",
    "        self.pre = 1 - self.gamma if use_prefactor else jnp.ones(N)\n",
    "        self.use_prefactor = use_prefactor\n",
    "\n",
    "        # Pre-compute a tensor for summing actions of other agents, for use in various computations.\n",
    "        self.Omega = self._OtherAgentsActionsSummationTensor()\n",
    "        self.has_last_statdist = False\n",
    "        self._last_statedist = jnp.ones(Z) / Z\n",
    "        \n",
    "        # Store the flag indicating whether to optimize einsum operations.\n",
    "        self.opti = opteinsum    \n",
    "\n",
    "    @partial(jit, static_argnums=0)\n",
    "    def Tss(self, policies_per_agent):\n",
    "        \"\"\"\n",
    "        In a game like MultipleObsSocialDilemma with simple transitions but multiple observations,\n",
    "        this method would theoretically map observations to agent policies and then to actions.\n",
    "        Since the game's state transitions are straightforward, the focus is on policy derivation.\n",
    "        \n",
    "        Args:\n",
    "            policies_per_agent (List[jnp.ndarray]): Policies derived from each agent's observations.\n",
    "        \"\"\"\n",
    "        # Assuming a single state, focus on how observations influence these policies.\n",
    "        # For a more complex game with actual state transitions based on observations:\n",
    "        \n",
    "        # Initialize an array for combined transitions (if more complex transitions were involved).\n",
    "        combined_transitions = jnp.zeros((self.n_states, self.n_states))\n",
    "        \n",
    "        for agent_idx, policy in enumerate(policies_per_agent):\n",
    "            # For each agent, derive how their observation influences their policy.\n",
    "            # Here, we directly use the policy as observation influences are embedded in policy derivation.\n",
    "            # In a more complex scenario, you'd map observations to states/actions here.\n",
    "            \n",
    "            for action_idx in range(self.n_agent_actions):\n",
    "                # Simplified as direct influence; in more complex scenarios, you'd use observation tensors.\n",
    "                action_probabilities = policy[:, action_idx]\n",
    "                transition_probs = self.transitions[:, action_idx, :]\n",
    "                \n",
    "                # Weight transition probabilities by action probabilities (assuming more complex transitions).\n",
    "                weighted_transitions = jnp.einsum('i,ij->ij', action_probabilities, transition_probs)\n",
    "                \n",
    "                # Aggregate transitions (if more than one state and complex transitions were involved).\n",
    "                combined_transitions += weighted_transitions / self.n_agent_actions\n",
    "        \n",
    "        # For MultipleObsSocialDilemma, the transition is inherently simple.\n",
    "        # The method above outlines how you might extend this for more complex scenarios.\n",
    "        return combined_transitions\n",
    "  \n",
    "    @partial(jit, static_argnums=0)    \n",
    "    def Ris(self,\n",
    "            Xisa:jnp.ndarray,  # Joint strategy array.\n",
    "            Risa:jnp.ndarray=None  # Optional pre-computed rewards for speed-up.\n",
    "           ) -> jnp.ndarray:  # Average reward for state transitions.\n",
    "        \"\"\"\n",
    "        Computes the average reward `Ris`, given a joint strategy `Xisa`.\n",
    "        This method calculates the expected rewards for state transitions, averaged\n",
    "        over the joint strategies of all agents, optionally using a pre-computed\n",
    "        reward tensor for efficiency.\n",
    "        \n",
    "        Args:\n",
    "            Xisa (jnp.ndarray): A joint strategy array.\n",
    "            Risa (jnp.ndarray, optional): An optional pre-computed reward tensor\n",
    "                to speed up calculations.\n",
    "                \n",
    "        Returns:\n",
    "            jnp.ndarray: An array representing the average rewards for state transitions\n",
    "                under the given joint strategy.\n",
    "        \"\"\"\n",
    "        if Risa is None:  # Calculate Ris from scratch if Risa is not provided.\n",
    "            # Variables for einsum computation.\n",
    "            i = 0; s = 1; sprim = 2; b2d = list(range(3, 3+self.N))\n",
    "        \n",
    "            # Prepare arguments for einsum operation.\n",
    "            X4einsum = list(it.chain(*zip(Xisa, [[s, b2d[a]] for a in range(self.N)])))\n",
    "\n",
    "            # Compute average rewards.\n",
    "            args = X4einsum + [self.T, [s]+b2d+[sprim], self.R, [i, s]+b2d+[sprim], [i, s]]\n",
    "            return jnp.einsum(*args, optimize=self.opti)\n",
    "        \n",
    "        else:  # Use pre-computed Risa to calculate Ris.\n",
    "            # Indices for agent, state, and action.\n",
    "            i=0; s=1; a=2\n",
    "            \n",
    "            # Perform einsum operation using pre-computed Risa.\n",
    "            args = [Xisa, [i, s, a], Risa, [i, s, a], [i, s]]\n",
    "            return jnp.einsum(*args, optimize=self.opti)\n",
    "\n",
    "       \n",
    "    @partial(jit, static_argnums=0)    \n",
    "    def Risa(self,\n",
    "             Xisa:jnp.ndarray  # Joint strategy array.\n",
    "            ) -> jnp.ndarray:  # Average reward for state-action pairs.\n",
    "        \"\"\"\n",
    "        Computes the average reward `Risa`, given a joint strategy `Xisa`.\n",
    "        This function determines the expected rewards for taking specific actions\n",
    "        in specific states, averaged over the joint strategies of all agents.\n",
    "        \n",
    "        Args:\n",
    "            Xisa (jnp.ndarray): A joint strategy array, indicating the probability\n",
    "                of each agent choosing each action in each state.\n",
    "                \n",
    "        Returns:\n",
    "            jnp.ndarray: An array representing the average rewards for state-action pairs\n",
    "                under the given joint strategy.\n",
    "        \"\"\"\n",
    "        # Variables for einsum computation.\n",
    "        i = 0; a = 1; s = 2; s_ = 3  \n",
    "        # Indices for other agents.\n",
    "        j2k = list(range(4, 4+self.N-1))  \n",
    "        # Indices for actions of all agents.\n",
    "        b2d = list(range(4+self.N-1, 4+self.N-1 + self.N))  \n",
    "        # Indices for actions of other agents.\n",
    "        e2f = list(range(3+2*self.N, 3+2*self.N + self.N-1))  \n",
    "\n",
    "        # Prepare indices for summation over other agents' actions.\n",
    "        sumsis = [[j2k[l], s, e2f[l]] for l in range(self.N-1)]  \n",
    "        otherX = list(it.chain(*zip((self.N-1)*[Xisa], sumsis)))\n",
    "\n",
    "        # Perform einsum operation to compute average rewards for state-action pairs.\n",
    "        args = [self.Omega, [i]+j2k+[a]+b2d+e2f] + otherX + [self.T, [s]+b2d+[s_], self.R, [i, s]+b2d+[s_], [i, s, a]]\n",
    "        return jnp.einsum(*args, optimize=self.opti)\n",
    "   \n",
    "       \n",
    "    @partial(jit, static_argnums=0)            \n",
    "    def Vis(self,\n",
    "            Xisa:jnp.ndarray,  # Joint strategy array.\n",
    "            Ris:jnp.ndarray=None,  # Optional average rewards for speed-up.\n",
    "            Tss:jnp.ndarray=None,  # Optional average transitions for speed-up.\n",
    "            Risa:jnp.ndarray=None  # Optional rewards for state-action pairs for speed-up.\n",
    "           ) -> jnp.ndarray:  # Average state values.\n",
    "        \"\"\"\n",
    "        Computes the average state values `Vis`, given a joint strategy `Xisa`.\n",
    "        This method calculates the value of being in each state, taking into account\n",
    "        the expected future rewards based on the joint strategy of all agents.\n",
    "        \n",
    "        Args:\n",
    "            Xisa (jnp.ndarray): A joint strategy array.\n",
    "            Ris (jnp.ndarray, optional): Pre-computed average rewards for state transitions.\n",
    "            Tss (jnp.ndarray, optional): Pre-computed average transition probabilities.\n",
    "            Risa (jnp.ndarray, optional): Pre-computed rewards for state-action pairs.\n",
    "                \n",
    "        Returns:\n",
    "            jnp.ndarray: An array representing the value of each state under the given\n",
    "                joint strategy.\n",
    "        \"\"\"\n",
    "        # Compute Ris and Tss if not provided.\n",
    "        Ris = self.Ris(Xisa, Risa=Risa) if Ris is None else Ris\n",
    "        Tss = self.Tss(Xisa) if Tss is None else Tss\n",
    "        \n",
    "        # Indices for agent and states.\n",
    "        i = 0; s = 1; sp = 2\n",
    "\n",
    "        # Compute the inverse of the matrix needed for solving the system of linear equations.\n",
    "        n = np.newaxis\n",
    "        Miss = np.eye(self.Z)[n,:,:] - self.gamma[:, n, n] * Tss[n,:,:]\n",
    "        invMiss = jnp.linalg.inv(Miss)\n",
    "        \n",
    "        # Solve the system of linear equations to find the state values.\n",
    "        return self.pre[:,n] * jnp.einsum(invMiss, [i, s, sp], Ris, [i, sp], [i, s], optimize=self.opti)\n",
    "\n",
    "\n",
    "    @partial(jit, static_argnums=0)        \n",
    "    def Qisa(self,\n",
    "             Xisa:jnp.ndarray,  # Joint strategy array.\n",
    "             Risa:jnp.ndarray=None,  # Optional rewards for speed-up.\n",
    "             Vis:jnp.ndarray=None,  # Optional state values for speed-up.\n",
    "             Tisas:jnp.ndarray=None  # Optional transitions for speed-up.\n",
    "            ) -> jnp.ndarray:  # Average state-action values.\n",
    "        \"\"\"\n",
    "        Computes the average state-action values `Qisa`, given a joint strategy `Xisa`.\n",
    "        This function estimates the value of taking specific actions in specific states,\n",
    "        considering the future rewards as influenced by the joint strategy of all agents.\n",
    "        \n",
    "        Args:\n",
    "            Xisa (jnp.ndarray): A joint strategy array.\n",
    "            Risa (jnp.ndarray, optional): Pre-computed rewards for state-action pairs.\n",
    "            Vis (jnp.ndarray, optional): Pre-computed values of states.\n",
    "            Tisas (jnp.ndarray, optional): Pre-computed average transitions from state-action pairs.\n",
    "                \n",
    "        Returns:\n",
    "            jnp.ndarray: An array representing the value of taking specific actions in\n",
    "                specific states under the given joint strategy.\n",
    "        \"\"\"\n",
    "        # Compute necessary components if not provided.\n",
    "        Risa = self.Risa(Xisa) if Risa is None else Risa\n",
    "        Vis = self.Vis(Xisa, Risa=Risa) if Vis is None else Vis\n",
    "        Tisas = self.Tisas(Xisa) if Tisas is None else Tisas\n",
    "\n",
    "        # Compute the expected future state values based on the action taken.\n",
    "        nextQisa = jnp.einsum(Tisas, [0,1,2,3], Vis, [0,3], [0,1,2], optimize=self.opti)\n",
    "\n",
    "        # Apply discounting and add immediate rewards to find state-action values.\n",
    "        n = np.newaxis\n",
    "        return self.pre[:,n,n] * Risa + self.gamma[:,n,n]*nextQisa\n",
    "\n",
    "    \n",
    "    \n",
    "    # === Helper ===\n",
    "    @partial(jit, static_argnums=0)  \n",
    "    def _jaxPs(self,\n",
    "               Xisa,  # Joint strategy\n",
    "               pS0):  # Last stationary state distribution \n",
    "        \"\"\"\n",
    "        Compute stationary distribution `Ps`, given joint strategy `Xisa`\n",
    "        using JAX.\n",
    "        \"\"\"\n",
    "        Tss = self.Tss(Xisa)\n",
    "        _pS = compute_stationarydistribution(Tss)\n",
    "        nrS = jnp.where(_pS.mean(0)!=-10, 1, 0).sum()\n",
    "\n",
    "        @jit\n",
    "        def single_dist(pS):\n",
    "            return jnp.max(jnp.where(_pS.mean(0)!=-10,\n",
    "                                     jnp.arange(_pS.shape[0]), -1))\n",
    "        @jit\n",
    "        def multi_dist(pS):\n",
    "            ix = jnp.argmin(jnp.linalg.norm(_pS.T - pS0, axis=-1))\n",
    "            return ix\n",
    "            \n",
    "        ix = jax.lax.cond(nrS == 1, single_dist, multi_dist, _pS)\n",
    "\n",
    "        pS = _pS[:, ix]\n",
    "        return pS\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy averaging\n",
    "Core methods to compute the strategy-average reward-prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L90){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Tss\n",
       "\n",
       ">      abase.Tss (Xisa:jax.Array)\n",
       "\n",
       "Computes the average transition model `Tss`, given a joint strategy `Xisa`.\n",
       "This method calculates how the environment's state is expected to change\n",
       "on average, given the current policies of all agents.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array, indicating the probability\n",
       "        of each agent choosing each action in each state.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the average transition probabilities\n",
       "        between states under the given joint strategy.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy as a numpy array. |\n",
       "| **Returns** | **Array** | **Returns the average transition matrix.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L90){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Tss\n",
       "\n",
       ">      abase.Tss (Xisa:jax.Array)\n",
       "\n",
       "Computes the average transition model `Tss`, given a joint strategy `Xisa`.\n",
       "This method calculates how the environment's state is expected to change\n",
       "on average, given the current policies of all agents.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array, indicating the probability\n",
       "        of each agent choosing each action in each state.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the average transition probabilities\n",
       "        between states under the given joint strategy.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy as a numpy array. |\n",
       "| **Returns** | **Array** | **Returns the average transition matrix.** |"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L120){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Tisas\n",
       "\n",
       ">      abase.Tisas (Xisa:jax.Array)\n",
       "\n",
       "Computes the average transition model `Tisas`, given a joint strategy `Xisa`.\n",
       "This function calculates the transition probabilities from state-action pairs\n",
       "to subsequent states, averaged over the joint strategies of all agents.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array, indicating the probability\n",
       "        of each agent choosing each action in each state.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the average transition probabilities\n",
       "        from state-action pairs to next states under the given joint strategy.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy array. |\n",
       "| **Returns** | **Array** | **Average transition Tisas.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L120){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Tisas\n",
       "\n",
       ">      abase.Tisas (Xisa:jax.Array)\n",
       "\n",
       "Computes the average transition model `Tisas`, given a joint strategy `Xisa`.\n",
       "This function calculates the transition probabilities from state-action pairs\n",
       "to subsequent states, averaged over the joint strategies of all agents.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array, indicating the probability\n",
       "        of each agent choosing each action in each state.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the average transition probabilities\n",
       "        from state-action pairs to next states under the given joint strategy.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy array. |\n",
       "| **Returns** | **Array** | **Average transition Tisas.** |"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Tisas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L160){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Ris\n",
       "\n",
       ">      abase.Ris (Xisa:jax.Array, Risa:jax.Array=None)\n",
       "\n",
       "Computes the average reward `Ris`, given a joint strategy `Xisa`.\n",
       "This method calculates the expected rewards for state transitions, averaged\n",
       "over the joint strategies of all agents, optionally using a pre-computed\n",
       "reward tensor for efficiency.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array.\n",
       "    Risa (jnp.ndarray, optional): An optional pre-computed reward tensor\n",
       "        to speed up calculations.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the average rewards for state transitions\n",
       "        under the given joint strategy.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy array. |\n",
       "| Risa | Array | None | Optional pre-computed rewards for speed-up. |\n",
       "| **Returns** | **Array** |  | **Average reward for state transitions.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L160){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Ris\n",
       "\n",
       ">      abase.Ris (Xisa:jax.Array, Risa:jax.Array=None)\n",
       "\n",
       "Computes the average reward `Ris`, given a joint strategy `Xisa`.\n",
       "This method calculates the expected rewards for state transitions, averaged\n",
       "over the joint strategies of all agents, optionally using a pre-computed\n",
       "reward tensor for efficiency.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array.\n",
       "    Risa (jnp.ndarray, optional): An optional pre-computed reward tensor\n",
       "        to speed up calculations.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the average rewards for state transitions\n",
       "        under the given joint strategy.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy array. |\n",
       "| Risa | Array | None | Optional pre-computed rewards for speed-up. |\n",
       "| **Returns** | **Array** |  | **Average reward for state transitions.** |"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Ris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L200){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Risa\n",
       "\n",
       ">      abase.Risa (Xisa:jax.Array)\n",
       "\n",
       "Computes the average reward `Risa`, given a joint strategy `Xisa`.\n",
       "This function determines the expected rewards for taking specific actions\n",
       "in specific states, averaged over the joint strategies of all agents.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array, indicating the probability\n",
       "        of each agent choosing each action in each state.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the average rewards for state-action pairs\n",
       "        under the given joint strategy.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy array. |\n",
       "| **Returns** | **Array** | **Average reward for state-action pairs.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L200){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Risa\n",
       "\n",
       ">      abase.Risa (Xisa:jax.Array)\n",
       "\n",
       "Computes the average reward `Risa`, given a joint strategy `Xisa`.\n",
       "This function determines the expected rewards for taking specific actions\n",
       "in specific states, averaged over the joint strategies of all agents.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array, indicating the probability\n",
       "        of each agent choosing each action in each state.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the average rewards for state-action pairs\n",
       "        under the given joint strategy.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy array. |\n",
       "| **Returns** | **Array** | **Average reward for state-action pairs.** |"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Risa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L235){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Vis\n",
       "\n",
       ">      abase.Vis (Xisa:jax.Array, Ris:jax.Array=None, Tss:jax.Array=None,\n",
       ">                 Risa:jax.Array=None)\n",
       "\n",
       "Computes the average state values `Vis`, given a joint strategy `Xisa`.\n",
       "This method calculates the value of being in each state, taking into account\n",
       "the expected future rewards based on the joint strategy of all agents.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array.\n",
       "    Ris (jnp.ndarray, optional): Pre-computed average rewards for state transitions.\n",
       "    Tss (jnp.ndarray, optional): Pre-computed average transition probabilities.\n",
       "    Risa (jnp.ndarray, optional): Pre-computed rewards for state-action pairs.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the value of each state under the given\n",
       "        joint strategy.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy array. |\n",
       "| Ris | Array | None | Optional average rewards for speed-up. |\n",
       "| Tss | Array | None | Optional average transitions for speed-up. |\n",
       "| Risa | Array | None | Optional rewards for state-action pairs for speed-up. |\n",
       "| **Returns** | **Array** |  | **Average state values.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L235){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Vis\n",
       "\n",
       ">      abase.Vis (Xisa:jax.Array, Ris:jax.Array=None, Tss:jax.Array=None,\n",
       ">                 Risa:jax.Array=None)\n",
       "\n",
       "Computes the average state values `Vis`, given a joint strategy `Xisa`.\n",
       "This method calculates the value of being in each state, taking into account\n",
       "the expected future rewards based on the joint strategy of all agents.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array.\n",
       "    Ris (jnp.ndarray, optional): Pre-computed average rewards for state transitions.\n",
       "    Tss (jnp.ndarray, optional): Pre-computed average transition probabilities.\n",
       "    Risa (jnp.ndarray, optional): Pre-computed rewards for state-action pairs.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the value of each state under the given\n",
       "        joint strategy.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy array. |\n",
       "| Ris | Array | None | Optional average rewards for speed-up. |\n",
       "| Tss | Array | None | Optional average transitions for speed-up. |\n",
       "| Risa | Array | None | Optional rewards for state-action pairs for speed-up. |\n",
       "| **Returns** | **Array** |  | **Average state values.** |"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L273){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Qisa\n",
       "\n",
       ">      abase.Qisa (Xisa:jax.Array, Risa:jax.Array=None, Vis:jax.Array=None,\n",
       ">                  Tisas:jax.Array=None)\n",
       "\n",
       "Computes the average state-action values `Qisa`, given a joint strategy `Xisa`.\n",
       "This function estimates the value of taking specific actions in specific states,\n",
       "considering the future rewards as influenced by the joint strategy of all agents.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array.\n",
       "    Risa (jnp.ndarray, optional): Pre-computed rewards for state-action pairs.\n",
       "    Vis (jnp.ndarray, optional): Pre-computed values of states.\n",
       "    Tisas (jnp.ndarray, optional): Pre-computed average transitions from state-action pairs.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the value of taking specific actions in\n",
       "        specific states under the given joint strategy.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy array. |\n",
       "| Risa | Array | None | Optional rewards for speed-up. |\n",
       "| Vis | Array | None | Optional state values for speed-up. |\n",
       "| Tisas | Array | None | Optional transitions for speed-up. |\n",
       "| **Returns** | **Array** |  | **Average state-action values.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L273){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Qisa\n",
       "\n",
       ">      abase.Qisa (Xisa:jax.Array, Risa:jax.Array=None, Vis:jax.Array=None,\n",
       ">                  Tisas:jax.Array=None)\n",
       "\n",
       "Computes the average state-action values `Qisa`, given a joint strategy `Xisa`.\n",
       "This function estimates the value of taking specific actions in specific states,\n",
       "considering the future rewards as influenced by the joint strategy of all agents.\n",
       "\n",
       "Args:\n",
       "    Xisa (jnp.ndarray): A joint strategy array.\n",
       "    Risa (jnp.ndarray, optional): Pre-computed rewards for state-action pairs.\n",
       "    Vis (jnp.ndarray, optional): Pre-computed values of states.\n",
       "    Tisas (jnp.ndarray, optional): Pre-computed average transitions from state-action pairs.\n",
       "\n",
       "Returns:\n",
       "    jnp.ndarray: An array representing the value of taking specific actions in\n",
       "        specific states under the given joint strategy.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy array. |\n",
       "| Risa | Array | None | Optional rewards for speed-up. |\n",
       "| Vis | Array | None | Optional state values for speed-up. |\n",
       "| Tisas | Array | None | Optional transitions for speed-up. |\n",
       "| **Returns** | **Array** |  | **Average state-action values.** |"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Qisa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def Ps(self:abase,\n",
    "       Xisa:jnp.ndarray # Joint strategy\n",
    "       ) -> jnp.ndarray: # Stationary state distribution\n",
    "    \"\"\"Compute stationary state distribution `Ps`, given joint strategy `Xisa`.\"\"\"\n",
    "    \n",
    "    # To make it work with JAX just-in-time compilation\n",
    "    if self.has_last_statdist: # Check whether we found a previous Ps\n",
    "        # If so, use jited computation\n",
    "        Ps =  self._jaxPs(Xisa, self._last_statedist)\n",
    "    else:\n",
    "        # If not, use the slower numpy implementation once\n",
    "        Ps = jnp.array(self._numpyPs(Xisa))\n",
    "        self.has_last_statdist = True\n",
    "\n",
    "    self._last_statedist = Ps\n",
    "    return Ps\n",
    "\n",
    "\n",
    "@patch\n",
    "def _numpyPs(self:abase, Xisa):\n",
    "    \"\"\"\n",
    "    Compute stationary distribution `Ps`, given joint strategy `Xisa`\n",
    "    just using numpy and without using JAX.\n",
    "    \"\"\"\n",
    "    Tss = self.Tss(Xisa)\n",
    "    _pS = np.array(compute_stationarydistribution(Tss))\n",
    "\n",
    "    # clean _pS from unwanted entries \n",
    "    _pS = _pS[:, _pS.mean(0)!=-10]\n",
    "    if len(_pS[0]) == 0:  # this happens when the tollerance can distinquish \n",
    "        assert False, 'No _statdist return - must not happen'\n",
    "    elif len(_pS[0]) > 1:  # Should not happen, in an ideal world\n",
    "        # sidenote: This means an ideal world is ergodic ;)\n",
    "        print(\"More than 1 state-eigenvector found\")\n",
    "\n",
    "        if hasattr(self, '_last_statedist'):  # if last exists\n",
    "            # take one that is closesd to last\n",
    "            # Sidenote: should also not happen, because for this case\n",
    "            # we are using the jitted implementation `_jaxPS`.\n",
    "            pS0 = self._last_statedist\n",
    "            choice = np.argmin(np.linalg.norm(_pS.T - pS0, axis=-1))\n",
    "            print('taking closest to last')\n",
    "        else: # if no last_Ps exists\n",
    "            # take a random one.\n",
    "            print(_pS.round(2))\n",
    "            nr = len(_pS[0])\n",
    "            choice = np.random.randint(nr)\n",
    "            print(\"taking random one: \", choice)\n",
    "        _pS = _pS[:, choice] \n",
    "        \n",
    "    return _pS.flatten() # clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ps` uses the `compute_stationarydistribution` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCRLD.Environments.EcologicalPublicGood import EcologicalPublicGood as EPG\n",
    "from pyCRLD.Agents.StrategyActorCritic import stratAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9028608 , 0.09713921], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = EPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01, degraded_choice=False)\n",
    "MAEi = stratAC(env=env, learning_rates=0.1, discount_factors=0.99, use_prefactor=True)\n",
    "\n",
    "x = MAEi.random_softmax_strategy()\n",
    "MAEi._numpyPs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.9028608 , 0.09713921], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEi.Ps(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def Ri(self:abase,\n",
    "       Xisa:jnp.ndarray # Joint strategy `Xisa`\n",
    "      ) -> jnp.ndarray: # Average reward `Ri`\n",
    "    \"\"\"Compute average reward `Ri`, given joint strategy `Xisa`.\"\"\" \n",
    "    i, s = 0, 1\n",
    "    return jnp.einsum(self.Ps(Xisa), [s], self.Ris(Xisa), [i, s], [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-4.574549 , -4.4456086], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEi.Ri(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def trajectory(self:abase,\n",
    "               Xinit:jnp.ndarray,  # Initial condition\n",
    "               Tmax:int=100, # the maximum number of iteration steps\n",
    "               tolerance:float=None, # to determine if a fix point is reached \n",
    "               verbose=False,  # Say something during computation?\n",
    "               **kwargs) -> tuple: # (`trajectory`, `fixpointreached`)\n",
    "    \"\"\"\n",
    "    Compute a joint learning trajectory.\n",
    "    \"\"\"\n",
    "    traj = []\n",
    "    t = 0\n",
    "    X = Xinit.copy()\n",
    "    fixpreached = False\n",
    "\n",
    "    while not fixpreached and t < Tmax:\n",
    "        print(f\"\\r [computing trajectory] step {t}\", end='') if verbose else None \n",
    "        traj.append(X)\n",
    "\n",
    "        X_, TDe = self.step(X)\n",
    "        if np.any(np.isnan(X_)):\n",
    "            fixpreached = True\n",
    "            break\n",
    "\n",
    "        if tolerance is not None:\n",
    "            fixpreached = np.linalg.norm(X_ - X) < tolerance\n",
    "\n",
    "        X = X_\n",
    "        t += 1\n",
    "\n",
    "    print(f\" [trajectory computed]\") if verbose else None\n",
    "\n",
    "    return np.array(traj), fixpreached"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trajectory` is an Array containing the time-evolution of the dynamic variable. \n",
    "`fixpointreached` is a bool saying whether or not a fixed point has been reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _OtherAgentsActionsSummationTensor(self:abase):\n",
    "    \"\"\"\n",
    "    To sum over the other agents and their respective actions using `einsum`.\n",
    "    \"\"\"\n",
    "    dim = np.concatenate(([self.N],  # agent i\n",
    "                          [self.N for _ in range(self.N-1)],  # other agnt\n",
    "                          [self.M],  # agent a of agent i\n",
    "                          [self.M for _ in range(self.N)],  # all acts\n",
    "                          [self.M for _ in range(self.N-1)]))  # other a's\n",
    "    Omega = np.zeros(dim.astype(int), int)\n",
    "\n",
    "    for index, _ in np.ndenumerate(Omega):\n",
    "        I = index[0]\n",
    "        notI = index[1:self.N]\n",
    "        A = index[self.N]\n",
    "        allA = index[self.N+1:2*self.N+1]\n",
    "        notA = index[2*self.N+1:]\n",
    "\n",
    "        if len(np.unique(np.concatenate(([I], notI)))) is self.N:\n",
    "            # all agents indices are different\n",
    "\n",
    "            if A == allA[I]:\n",
    "                # action of agent i equals some other action\n",
    "                cd = allA[:I] + allA[I+1:]  # other actionss\n",
    "                areequal = [cd[k] == notA[k] for k in range(self.N-1)]\n",
    "                if np.all(areequal):\n",
    "                    Omega[index] = 1\n",
    "\n",
    "    return jnp.array(Omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L436){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase._OtherAgentsActionsSummationTensor\n",
       "\n",
       ">      abase._OtherAgentsActionsSummationTensor ()\n",
       "\n",
       "To sum over the other agents and their respective actions using `einsum`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L436){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase._OtherAgentsActionsSummationTensor\n",
       "\n",
       ">      abase._OtherAgentsActionsSummationTensor ()\n",
       "\n",
       "To sum over the other agents and their respective actions using `einsum`."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase._OtherAgentsActionsSummationTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the strategy-average reward-prediction error for agent $i$, we need to average out the probabilities contained in the strategies of all other agents $j \\neq i$ and the  transition function $T$, \n",
    "\n",
    "$$\n",
    "\\sum_{a^j} \\sum_{s'} \\prod_{i\\neq j} X^j(s, a^j) T(s, \\mathbf a, s').\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_OtherAgentsActionsSummationTensor` enables this summation to be exectued in the efficient `einsum` function. It contains only $0$s and $1$s and is of dimension \n",
    "\n",
    "$$\n",
    "N \\times \\underbrace{N \\times ... \\times N}_{(N-1) \\text{ times}}\n",
    "\\times M \\times \\underbrace{M \\times ... \\times M}_{N \\text{ times}}\n",
    "\\times \\underbrace{M \\times ... \\times M}_{(N-1) \\text{ times}}\n",
    "$$\n",
    "\n",
    "which represent\n",
    "\n",
    "$$\n",
    "\\overbrace{N}^{\\text{the focal agent}} \n",
    "\\times \n",
    "\\overbrace{\\underbrace{N \\times ... \\times N}_{(N-1) \\text{ times}}}^\\text{all other agents}\n",
    "\\times \n",
    "\\overbrace{M}^\\text{focal agent's action} \n",
    "\\times \n",
    "\\overbrace{\\underbrace{M \\times ... \\times M}_{N \\text{ times}}}^\\text{all actions}\n",
    "\\times \n",
    "\\overbrace{\\underbrace{M \\times ... \\times M}_{(N-1) \\text{ times}}}^\\text{all other agents' actions}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains a $1$ only if\n",
    "\n",
    "* all agent indices (comprised of the *focal agent* index and *all other agents* indices) are different from each other\n",
    "* and the *focal agent's action* index matches the focal agents' action index in *all actions* \n",
    "* and if *all other agents' action* indices match their corresponding action indices in *all actions*.\n",
    "\n",
    "Otherwise it contains a $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

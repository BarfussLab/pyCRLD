{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base\n",
    "\n",
    "> Base class containing the core methods of CRLD agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Agents/Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from typing import Iterable\n",
    "from fastcore.utils import *\n",
    "\n",
    "from pyCRLD.Utils.Helpers import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The agent base class\n",
    "contains core methods to compute the strategy-average reward-prediction error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class abase(object):\n",
    "    \"\"\"\n",
    "    Base class for deterministic strategy-average independent (multi-agent)\n",
    "    temporal-difference reinforcement learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 TransitionTensor: np.ndarray, # transition model of the environment\n",
    "                 RewardTensor: np.ndarray,  # reward model of the environment\n",
    "                 DiscountFactors: Iterable[float],  # the agents' discount factors\n",
    "                 use_prefactor=False,  # use the 1-DiscountFactor prefactor\n",
    "                 opteinsum=True):  # optimize einsum functions\n",
    "                \n",
    "        R = jnp.array(RewardTensor)\n",
    "        T = jnp.array(TransitionTensor)\n",
    "    \n",
    "        # number of agents\n",
    "        N = R.shape[0]  \n",
    "        assert len(T.shape[1:-1]) == N, \"Inconsistent number of agents\"\n",
    "        assert len(R.shape[2:-1]) == N, \"Inconsistent number of agents\"\n",
    "        \n",
    "        # number of actions for each agent        \n",
    "        M = T.shape[1] \n",
    "        assert np.allclose(T.shape[1:-1], M), 'Inconsisten number of actions'\n",
    "        assert np.allclose(R.shape[2:-1], M), 'Inconsisten number of actions'\n",
    "        \n",
    "        # number of states\n",
    "        Z = T.shape[0] \n",
    "        assert T.shape[-1] == Z, 'Inconsisten number of states'\n",
    "        assert R.shape[-1] == Z, 'Inconsisten number of states'\n",
    "        assert R.shape[1] == Z, 'Inconsisten number of states'\n",
    "        \n",
    "        self.R, self.T, self.N, self.M, self.Z, self.Q = R, T, N, M, Z, Z\n",
    "        \n",
    "        # discount factors\n",
    "        self.gamma = make_variable_vector(DiscountFactors, N)\n",
    "\n",
    "        # use (1-DiscountFactor) prefactor to have values on scale of rewards\n",
    "        self.pre = 1 - self.gamma if use_prefactor else jnp.ones(N)        \n",
    "        self.use_prefactor = use_prefactor\n",
    "\n",
    "        # 'load' the other agents actions summation tensor for speed\n",
    "        self.Omega = self._OtherAgentsActionsSummationTensor()\n",
    "        self.has_last_statdist = False\n",
    "        self._last_statedist = jnp.ones(Z) / Z\n",
    "        \n",
    "        # use optimized einsum method\n",
    "        self.opti = opteinsum  \n",
    "\n",
    "    @partial(jit, static_argnums=0)    \n",
    "    def Tss(self, \n",
    "            Xisa:jnp.ndarray  # Joint strategy\n",
    "           ) -> jnp.ndarray: # Average transition matrix\n",
    "        \"\"\"Compute average transition model `Tss`, given joint strategy `Xisa`\"\"\"\n",
    "        # i = 0  # agent i (not needed)\n",
    "        s = 1  # state s\n",
    "        sprim = 2  # next state s'\n",
    "        b2d = list(range(3, 3+self.N))  # all actions\n",
    "\n",
    "        X4einsum = list(it.chain(*zip(Xisa, [[s, b2d[a]] for a in range(self.N)])))\n",
    "        args = X4einsum + [self.T, [s]+b2d+[sprim], [s, sprim]]\n",
    "        return jnp.einsum(*args, optimize=self.opti)\n",
    "    \n",
    "    @partial(jit, static_argnums=0)    \n",
    "    def Tisas(self,\n",
    "              Xisa:jnp.ndarray  # Joint strategy\n",
    "             ) -> jnp.ndarray:  #  Average transition Tisas\n",
    "        \"\"\"Compute average transition model `Tisas`, given joint strategy `Xisa`\"\"\"      \n",
    "        i = 0  # agent i\n",
    "        a = 1  # its action a\n",
    "        s = 2  # the current state\n",
    "        s_ = 3  # the next state\n",
    "        j2k = list(range(4, 4+self.N-1))  # other agents\n",
    "        b2d = list(range(4+self.N-1, 4+self.N-1 + self.N))  # all actions\n",
    "        e2f = list(range(3+2*self.N, 3+2*self.N + self.N-1))  # all other acts\n",
    "\n",
    "        sumsis = [[j2k[l], s, e2f[l]] for l in range(self.N-1)]  # sum inds\n",
    "        otherX = list(it.chain(*zip((self.N-1)*[Xisa], sumsis)))\n",
    "\n",
    "        args = [self.Omega, [i]+j2k+[a]+b2d+e2f] + otherX\\\n",
    "            + [self.T, [s]+b2d+[s_], [i, s, a, s_]]\n",
    "        return jnp.einsum(*args, optimize=self.opti)\n",
    "\n",
    "    @partial(jit, static_argnums=0)    \n",
    "    def Ris(self,\n",
    "            Xisa:jnp.ndarray, # Joint strategy\n",
    "            Risa:jnp.ndarray=None # Optional reward for speed-up\n",
    "           ) -> jnp.ndarray: # Average reward\n",
    "        \"\"\"Compute average reward `Ris`, given joint strategy `Xisa`\"\"\" \n",
    "        if Risa is None:  # for speed up\n",
    "            # Variables      \n",
    "            i = 0; s = 1; sprim = 2; b2d = list(range(3, 3+self.N))\n",
    "        \n",
    "            X4einsum = list(it.chain(*zip(Xisa,\n",
    "                                    [[s, b2d[a]] for a in range(self.N)])))\n",
    "\n",
    "            args = X4einsum + [self.T, [s]+b2d+[sprim],\n",
    "                               self.R, [i, s]+b2d+[sprim], [i, s]]\n",
    "            return jnp.einsum(*args, optimize=self.opti)\n",
    "        \n",
    "        else:  # Compute Ris from Risa \n",
    "            i=0; s=1; a=2\n",
    "            args = [Xisa, [i, s, a], Risa, [i, s, a], [i, s]]\n",
    "            return jnp.einsum(*args, optimize=self.opti)\n",
    "       \n",
    "    @partial(jit, static_argnums=0)    \n",
    "    def Risa(self,\n",
    "             Xisa:jnp.ndarray # Joint strategy\n",
    "            ) -> jnp.ndarray:  # Average reward\n",
    "        \"\"\"Compute average reward `Risa`, given joint strategy `Xisa`\"\"\"\n",
    "        i = 0; a = 1; s = 2; s_ = 3  # Variables\n",
    "        j2k = list(range(4, 4+self.N-1))  # other agents\n",
    "        b2d = list(range(4+self.N-1, 4+self.N-1 + self.N))  # all actions\n",
    "        e2f = list(range(3+2*self.N, 3+2*self.N + self.N-1))  # all other acts\n",
    " \n",
    "        sumsis = [[j2k[l], s, e2f[l]] for l in range(self.N-1)]  # sum inds\n",
    "        otherX = list(it.chain(*zip((self.N-1)*[Xisa], sumsis)))\n",
    "\n",
    "        args = [self.Omega, [i]+j2k+[a]+b2d+e2f] + otherX\\\n",
    "            + [self.T, [s]+b2d+[s_], self.R, [i, s]+b2d+[s_],\n",
    "               [i, s, a]]\n",
    "        return jnp.einsum(*args, optimize=self.opti)       \n",
    "       \n",
    "    @partial(jit, static_argnums=0)            \n",
    "    def Vis(self,\n",
    "            Xisa:jnp.ndarray, # Joint strategy\n",
    "            Ris:jnp.ndarray=None, # Optional reward for speed-up\n",
    "            Tss:jnp.ndarray=None, # Optional transition for speed-up\n",
    "            Risa:jnp.ndarray=None  # Optional reward for speed-up\n",
    "           ) -> jnp.ndarray:  # Average state values\n",
    "        \"\"\"Compute average state values `Vis`, given joint strategy `Xisa`\"\"\"\n",
    "        # For speed up\n",
    "        Ris = self.Ris(Xisa, Risa=Risa) if Ris is None else Ris\n",
    "        Tss = self.Tss(Xisa) if Tss is None else Tss\n",
    "        \n",
    "        i = 0  # agent i\n",
    "        s = 1  # state s\n",
    "        sp = 2  # next state s'\n",
    "\n",
    "        n = np.newaxis\n",
    "        Miss = np.eye(self.Z)[n,:,:] - self.gamma[:, n, n] * Tss[n,:,:]\n",
    "        \n",
    "        invMiss = jnp.linalg.inv(Miss)\n",
    "               \n",
    "        return self.pre[:,n] * jnp.einsum(invMiss, [i, s, sp], Ris, [i, sp],\n",
    "                                          [i, s], optimize=self.opti)\n",
    "\n",
    "    @partial(jit, static_argnums=0)        \n",
    "    def Qisa(self,\n",
    "             Xisa:jnp.ndarray, # Joint strategy\n",
    "             Risa:jnp.ndarray=None, #  Optional reward for speed-up\n",
    "             Vis:jnp.ndarray=None, # Optional values for speed-up\n",
    "             Tisas:jnp.ndarray=None, # Optional transition for speed-up\n",
    "            ) -> jnp.ndarray:  # Average state-action values\n",
    "        \"\"\"Compute average state-action values Qisa, given joint strategy `Xisa`\"\"\"\n",
    "        # For speed up\n",
    "        Risa = self.Risa(Xisa) if Risa is None else Risa\n",
    "        Vis = self.Vis(Xisa, Risa=Risa) if Vis is None else Vis\n",
    "        Tisas = self.Tisas(Xisa) if Tisas is None else Tisas\n",
    "\n",
    "        nextQisa = jnp.einsum(Tisas, [0,1,2,3], Vis, [0,3], [0,1,2],\n",
    "                              optimize=self.opti)\n",
    "\n",
    "        n = np.newaxis\n",
    "        return self.pre[:,n,n] * Risa + self.gamma[:,n,n]*nextQisa\n",
    "    \n",
    "    \n",
    "    # === Helper ===\n",
    "    @partial(jit, static_argnums=0)  \n",
    "    def _jaxPs(self,\n",
    "               Xisa,  # Joint strategy\n",
    "               pS0):  # Last stationary state distribution \n",
    "        \"\"\"\n",
    "        Compute stationary distribution `Ps`, given joint strategy `Xisa`\n",
    "        using JAX.\n",
    "        \"\"\"\n",
    "        Tss = self.Tss(Xisa)\n",
    "        _pS = compute_stationarydistribution(Tss)\n",
    "        nrS = jnp.where(_pS.mean(0)!=-10, 1, 0).sum()\n",
    "\n",
    "        @jit\n",
    "        def single_dist(pS):\n",
    "            return jnp.max(jnp.where(_pS.mean(0)!=-10,\n",
    "                                     jnp.arange(_pS.shape[0]), -1))\n",
    "        @jit\n",
    "        def multi_dist(pS):\n",
    "            ix = jnp.argmin(jnp.linalg.norm(_pS.T - pS0, axis=-1))\n",
    "            return ix\n",
    "            \n",
    "        ix = jax.lax.cond(nrS == 1, single_dist, multi_dist, _pS)\n",
    "\n",
    "        pS = _pS[:, ix]\n",
    "        return pS\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy averaging\n",
    "Core methods to compute the strategy-average reward-prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L71){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Tss\n",
       "\n",
       ">      abase.Tss (Xisa:jax.Array)\n",
       "\n",
       "Compute average transition model `Tss`, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy |\n",
       "| **Returns** | **Array** | **Average transition matrix** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L71){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Tss\n",
       "\n",
       ">      abase.Tss (Xisa:jax.Array)\n",
       "\n",
       "Compute average transition model `Tss`, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy |\n",
       "| **Returns** | **Array** | **Average transition matrix** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Tisas\n",
       "\n",
       ">      abase.Tisas (Xisa:jax.Array)\n",
       "\n",
       "Compute average transition model `Tisas`, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy |\n",
       "| **Returns** | **Array** | **Average transition Tisas** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Tisas\n",
       "\n",
       ">      abase.Tisas (Xisa:jax.Array)\n",
       "\n",
       "Compute average transition model `Tisas`, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy |\n",
       "| **Returns** | **Array** | **Average transition Tisas** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Tisas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L105){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Ris\n",
       "\n",
       ">      abase.Ris (Xisa:jax.Array, Risa:jax.Array=None)\n",
       "\n",
       "Compute average reward `Ris`, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy |\n",
       "| Risa | Array | None | Optional reward for speed-up |\n",
       "| **Returns** | **Array** |  | **Average reward** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L105){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Ris\n",
       "\n",
       ">      abase.Ris (Xisa:jax.Array, Risa:jax.Array=None)\n",
       "\n",
       "Compute average reward `Ris`, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy |\n",
       "| Risa | Array | None | Optional reward for speed-up |\n",
       "| **Returns** | **Array** |  | **Average reward** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Ris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Risa\n",
       "\n",
       ">      abase.Risa (Xisa:jax.Array)\n",
       "\n",
       "Compute average reward `Risa`, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy |\n",
       "| **Returns** | **Array** | **Average reward** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L127){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Risa\n",
       "\n",
       ">      abase.Risa (Xisa:jax.Array)\n",
       "\n",
       "Compute average reward `Risa`, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| Xisa | Array | Joint strategy |\n",
       "| **Returns** | **Array** | **Average reward** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Risa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L145){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Vis\n",
       "\n",
       ">      abase.Vis (Xisa:jax.Array, Ris:jax.Array=None, Tss:jax.Array=None,\n",
       ">                 Risa:jax.Array=None)\n",
       "\n",
       "Compute average state values `Vis`, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy |\n",
       "| Ris | Array | None | Optional reward for speed-up |\n",
       "| Tss | Array | None | Optional transition for speed-up |\n",
       "| Risa | Array | None | Optional reward for speed-up |\n",
       "| **Returns** | **Array** |  | **Average state values** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L145){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Vis\n",
       "\n",
       ">      abase.Vis (Xisa:jax.Array, Ris:jax.Array=None, Tss:jax.Array=None,\n",
       ">                 Risa:jax.Array=None)\n",
       "\n",
       "Compute average state values `Vis`, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy |\n",
       "| Ris | Array | None | Optional reward for speed-up |\n",
       "| Tss | Array | None | Optional transition for speed-up |\n",
       "| Risa | Array | None | Optional reward for speed-up |\n",
       "| **Returns** | **Array** |  | **Average state values** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L169){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Qisa\n",
       "\n",
       ">      abase.Qisa (Xisa:jax.Array, Risa:jax.Array=None, Vis:jax.Array=None,\n",
       ">                  Tisas:jax.Array=None)\n",
       "\n",
       "Compute average state-action values Qisa, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy |\n",
       "| Risa | Array | None | Optional reward for speed-up |\n",
       "| Vis | Array | None | Optional values for speed-up |\n",
       "| Tisas | Array | None | Optional transition for speed-up |\n",
       "| **Returns** | **Array** |  | **Average state-action values** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L169){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase.Qisa\n",
       "\n",
       ">      abase.Qisa (Xisa:jax.Array, Risa:jax.Array=None, Vis:jax.Array=None,\n",
       ">                  Tisas:jax.Array=None)\n",
       "\n",
       "Compute average state-action values Qisa, given joint strategy `Xisa`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| Xisa | Array |  | Joint strategy |\n",
       "| Risa | Array | None | Optional reward for speed-up |\n",
       "| Vis | Array | None | Optional values for speed-up |\n",
       "| Tisas | Array | None | Optional transition for speed-up |\n",
       "| **Returns** | **Array** |  | **Average state-action values** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase.Qisa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def Ps(self:abase,\n",
    "       Xisa:jnp.ndarray # Joint strategy\n",
    "       ) -> jnp.ndarray: # Stationary state distribution\n",
    "    \"\"\"Compute stationary state distribution `Ps`, given joint strategy `Xisa`.\"\"\"\n",
    "    \n",
    "    # To make it work with JAX just-in-time compilation\n",
    "    if self.has_last_statdist: # Check whether we found a previous Ps\n",
    "        # If so, use jited computation\n",
    "        Ps =  self._jaxPs(Xisa, self._last_statedist)\n",
    "    else:\n",
    "        # If not, use the slower numpy implementation once\n",
    "        Ps = jnp.array(self._numpyPs(Xisa))\n",
    "        self.has_last_statdist = True\n",
    "\n",
    "    self._last_statedist = Ps\n",
    "    return Ps\n",
    "\n",
    "\n",
    "@patch\n",
    "def _numpyPs(self:abase, Xisa):\n",
    "    \"\"\"\n",
    "    Compute stationary distribution `Ps`, given joint strategy `Xisa`\n",
    "    just using numpy and without using JAX.\n",
    "    \"\"\"\n",
    "    Tss = self.Tss(Xisa)\n",
    "    _pS = np.array(compute_stationarydistribution(Tss))\n",
    "\n",
    "    # clean _pS from unwanted entries \n",
    "    _pS = _pS[:, _pS.mean(0)!=-10]\n",
    "    if len(_pS[0]) == 0:  # this happens when the tollerance can distinquish \n",
    "        assert False, 'No _statdist return - must not happen'\n",
    "    elif len(_pS[0]) > 1:  # Should not happen, in an ideal world\n",
    "        # sidenote: This means an ideal world is ergodic ;)\n",
    "        print(\"More than 1 state-eigenvector found\")\n",
    "\n",
    "        if hasattr(self, '_last_statedist'):  # if last exists\n",
    "            # take one that is closesd to last\n",
    "            # Sidenote: should also not happen, because for this case\n",
    "            # we are using the jitted implementation `_jaxPS`.\n",
    "            pS0 = self._last_statedist\n",
    "            choice = np.argmin(np.linalg.norm(_pS.T - pS0, axis=-1))\n",
    "            print('taking closest to last')\n",
    "        else: # if no last_Ps exists\n",
    "            # take a random one.\n",
    "            print(_pS.round(2))\n",
    "            nr = len(_pS[0])\n",
    "            choice = np.random.randint(nr)\n",
    "            print(\"taking random one: \", choice)\n",
    "        _pS = _pS[:, choice] \n",
    "        \n",
    "    return _pS.flatten() # clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ps` uses the `compute_stationarydistribution` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCRLD.Environments.EcologicalPublicGood import EcologicalPublicGood as EPG\n",
    "from pyCRLD.Agents.StrategyActorCritic import stratAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93960035, 0.06039964], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = EPG(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.01, degraded_choice=False)\n",
    "MAEi = stratAC(env=env, learning_rates=0.1, discount_factors=0.99, use_prefactor=True)\n",
    "\n",
    "x = MAEi.random_softmax_strategy()\n",
    "MAEi._numpyPs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.93960035, 0.06039964], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEi.Ps(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def Ri(self:abase,\n",
    "       Xisa:jnp.ndarray # Joint strategy `Xisa`\n",
    "      ) -> jnp.ndarray: # Average reward `Ri`\n",
    "    \"\"\"Compute average reward `Ri`, given joint strategy `Xisa`.\"\"\" \n",
    "    i, s = 0, 1\n",
    "    return jnp.einsum(self.Ps(Xisa), [s], self.Ris(Xisa), [i, s], [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-4.70174  , -4.7636294], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAEi.Ri(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def trajectory(self:abase,\n",
    "               Xinit:jnp.ndarray,  # Initial condition\n",
    "               Tmax:int=100, # the maximum number of iteration steps\n",
    "               tolerance:float=None, # to determine if a fix point is reached \n",
    "               verbose=False,  # Say something during computation?\n",
    "               **kwargs) -> tuple: # (`trajectory`, `fixpointreached`)\n",
    "    \"\"\"\n",
    "    Compute a joint learning trajectory.\n",
    "    \"\"\"\n",
    "    traj = []\n",
    "    t = 0\n",
    "    X = Xinit.copy()\n",
    "    fixpreached = False\n",
    "\n",
    "    while not fixpreached and t < Tmax:\n",
    "        print(f\"\\r [computing trajectory] step {t}\", end='') if verbose else None \n",
    "        traj.append(X)\n",
    "\n",
    "        X_, TDe = self.step(X)\n",
    "        if np.any(np.isnan(X_)):\n",
    "            fixpreached = True\n",
    "            break\n",
    "\n",
    "        if tolerance is not None:\n",
    "            fixpreached = np.linalg.norm(X_ - X) < tolerance\n",
    "\n",
    "        X = X_\n",
    "        t += 1\n",
    "\n",
    "    print(f\" [trajectory computed]\") if verbose else None\n",
    "\n",
    "    return np.array(traj), fixpreached"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trajectory` is an Array containing the time-evolution of the dynamic variable. \n",
    "`fixpointreached` is a bool saying whether or not a fixed point has been reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _OtherAgentsActionsSummationTensor(self:abase):\n",
    "    \"\"\"\n",
    "    To sum over the other agents and their respective actions using `einsum`.\n",
    "    \"\"\"\n",
    "    dim = np.concatenate(([self.N],  # agent i\n",
    "                          [self.N for _ in range(self.N-1)],  # other agnt\n",
    "                          [self.M],  # agent a of agent i\n",
    "                          [self.M for _ in range(self.N)],  # all acts\n",
    "                          [self.M for _ in range(self.N-1)]))  # other a's\n",
    "    Omega = np.zeros(dim.astype(int), int)\n",
    "\n",
    "    for index, _ in np.ndenumerate(Omega):\n",
    "        I = index[0]\n",
    "        notI = index[1:self.N]\n",
    "        A = index[self.N]\n",
    "        allA = index[self.N+1:2*self.N+1]\n",
    "        notA = index[2*self.N+1:]\n",
    "\n",
    "        if len(np.unique(np.concatenate(([I], notI)))) is self.N:\n",
    "            # all agents indices are different\n",
    "\n",
    "            if A == allA[I]:\n",
    "                # action of agent i equals some other action\n",
    "                cd = allA[:I] + allA[I+1:]  # other actionss\n",
    "                areequal = [cd[k] == notA[k] for k in range(self.N-1)]\n",
    "                if np.all(areequal):\n",
    "                    Omega[index] = 1\n",
    "\n",
    "    return jnp.array(Omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L316){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase._OtherAgentsActionsSummationTensor\n",
       "\n",
       ">      abase._OtherAgentsActionsSummationTensor ()\n",
       "\n",
       "To sum over the other agents and their respective actions using `einsum`."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/wbarfuss/pyCRLD/blob/main/pyCRLD/Agents/Base.py#L316){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### abase._OtherAgentsActionsSummationTensor\n",
       "\n",
       ">      abase._OtherAgentsActionsSummationTensor ()\n",
       "\n",
       "To sum over the other agents and their respective actions using `einsum`."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(abase._OtherAgentsActionsSummationTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the strategy-average reward-prediction error for agent $i$, we need to average out the probabilities contained in the strategies of all other agents $j \\neq i$ and the  transition function $T$, \n",
    "\n",
    "$$\n",
    "\\sum_{a^j} \\sum_{s'} \\prod_{i\\neq j} X^j(s, a^j) T(s, \\mathbf a, s').\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_OtherAgentsActionsSummationTensor` enables this summation to be exectued in the efficient `einsum` function. It contains only $0$s and $1$s and is of dimension \n",
    "\n",
    "$$\n",
    "N \\times \\underbrace{N \\times ... \\times N}_{(N-1) \\text{ times}}\n",
    "\\times M \\times \\underbrace{M \\times ... \\times M}_{N \\text{ times}}\n",
    "\\times \\underbrace{M \\times ... \\times M}_{(N-1) \\text{ times}}\n",
    "$$\n",
    "\n",
    "which represent\n",
    "\n",
    "$$\n",
    "\\overbrace{N}^{\\text{the focal agent}} \n",
    "\\times \n",
    "\\overbrace{\\underbrace{N \\times ... \\times N}_{(N-1) \\text{ times}}}^\\text{all other agents}\n",
    "\\times \n",
    "\\overbrace{M}^\\text{focal agent's action} \n",
    "\\times \n",
    "\\overbrace{\\underbrace{M \\times ... \\times M}_{N \\text{ times}}}^\\text{all actions}\n",
    "\\times \n",
    "\\overbrace{\\underbrace{M \\times ... \\times M}_{(N-1) \\text{ times}}}^\\text{all other agents' actions}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains a $1$ only if\n",
    "\n",
    "* all agent indices (comprised of the *focal agent* index and *all other agents* indices) are different from each other\n",
    "* and the *focal agent's action* index matches the focal agents' action index in *all actions* \n",
    "* and if *all other agents' action* indices match their corresponding action indices in *all actions*.\n",
    "\n",
    "Otherwise it contains a $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

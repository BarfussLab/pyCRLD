{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adb73f99-6be9-48a9-83ca-2cd045d3a8be",
   "metadata": {},
   "source": [
    "# Strategy SARSA\n",
    "\n",
    "> CRLD SARSA agents in strategy space  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b26bb25-518a-47df-8656-6ab3cc792e81",
   "metadata": {},
   "source": [
    "SARSA agents take into acount the five pieces of information of current State, current Action, Reward, next State and next Action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055fee3-2b02-4710-9ee4-5394621da31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Agents/StrategySARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92234412-f8cf-4b25-afc1-1fc4c2257da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5646f-3771-4c7c-b799-c17fe7afc555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23ce7582-e8af-440e-8b07-0f90ab37ed6a",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e727f-42c4-4a93-9b21-e7cb961da565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCRLD.Agents.StrategySARSA import stratSARSA\n",
    "from pyCRLD.Agents.StrategyActorCritic import stratAC\n",
    "\n",
    "from pyCRLD.Environments.SocialDilemma import SocialDilemma\n",
    "from pyCRLD.Utils import FlowPlot as fp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad2f36-1da3-4c48-b371-b92ed6771535",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SocialDilemma(R=1.0, T=0.8, S=-0.5, P=0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db6e80db-fbf4-4192-a66c-8d54a31b3d86",
   "metadata": {},
   "source": [
    "Let's compare the SARSA (in red) with the actor-critic learners (in blue). The difference is that the SARSA learners incorporate an explicit exploration term in their learning update, regulated by the `choice_intensities`. For low choice intensities, the SARSA learners tend to extreme exploration, i.e., toward the center of the strategy space. For high choice intensities, the SARSA map onto the actor-critic learners (see Figure below). For the actor-critic learners, the `choice_intensities` have no effect other than scaling the learning speed alongside the learning rates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0accc-c92e-4b19-b42c-6c6ddc3bd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize=(18,4))\n",
    "faps = np.linspace(0.01 ,0.99, 9)\n",
    "x = ([0], [0], [0])\n",
    "y = ([1], [0], [0])\n",
    "\n",
    "for i, ci in enumerate([0.1, 1.0, 10, 100]):\n",
    "\n",
    "    maeiAC = stratAC(env=env, learning_rates=0.1, discount_factors=0.9, choice_intensities=ci)\n",
    "    maeiSARSA = stratSARSA(env=env, learning_rates=0.1, discount_factors=0.9, choice_intensities=ci)\n",
    "\n",
    "    fp.plot_strategy_flow(maeiAC, x, y, flowarrow_points=faps, cmap=\"Blues\", axes=[ax[i]])\n",
    "    fp.plot_strategy_flow(maeiSARSA, x, y, flowarrow_points=faps, cmap=\"Reds\", axes=[ax[i]]);\n",
    "\n",
    "    ax[i].set_xlabel(\"Agent 0's cooperation probability\")\n",
    "    ax[i].set_ylabel(\"Agent 1's cooperation probability\")\n",
    "    ax[i].set_title(\"Intensity of choice {}\".format(ci));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fae43c49-2e63-466a-9df1-0be689bae075",
   "metadata": {},
   "source": [
    "## API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fe169-b091-4317-9206-7fc0371e6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from fastcore.utils import *\n",
    "\n",
    "from pyCRLD.Agents.StrategyBase import strategybase\n",
    "from pyCRLD.Utils.Helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2301d-b92d-4b02-9fc1-9b56bf6a97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export   \n",
    "class stratSARSA(strategybase):\n",
    "    \"\"\"\n",
    "    Class for CRLD-SARSA agents in strategy space.\n",
    "    \"\"\"\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,2))\n",
    "    def RPEisa(self,\n",
    "               Xisa,  # Joint strategy\n",
    "               norm=False # normalize error around actions? \n",
    "               ) -> np.ndarray:  # RP/TD error\n",
    "        \"\"\"\n",
    "        Compute reward-prediction/temporal-difference error for \n",
    "        strategy SARSA dynamics, given joint strategy `Xisa`.\n",
    "        \"\"\"\n",
    "        R = self.Risa(Xisa)\n",
    "        NextQ = self.NextQisa(Xisa, Risa=R)\n",
    "\n",
    "        n = jnp.newaxis\n",
    "        E = self.pre[:,n,n]*R + self.gamma[:,n,n]*NextQ - 1/self.beta[:, n, n] * jnp.log(Xisa)\n",
    "        E *= self.beta[:,n,n]\n",
    "\n",
    "        E = E - E.mean(axis=2, keepdims=True) if norm else E\n",
    "        return E\n",
    "    \n",
    "    @partial(jit, static_argnums=0)\n",
    "    def NextQisa(self,\n",
    "                 Xisa,          # Joint strategy\n",
    "                 Qisa=None,  # Optional state-action values for speed-up\n",
    "                 Risa=None,  # Optional rewards for speed-up\n",
    "                 Vis=None,   # Optional state values for speed-up\n",
    "                 Tisas=None  # Optional transition for speed-up\n",
    "                ) -> jnp.ndarray: # Next values       \n",
    "        \"\"\"\n",
    "        Compute strategy-average next state-action value for agent `i`, current state `s` and action `a`.\n",
    "        \"\"\"\n",
    "        Qisa = self.Qisa(Xisa, Risa=None, Vis=None, Tisas=None)\\\n",
    "            if Qisa is None else Qisa\n",
    "        \n",
    "        i = 0; a = 1; s = 2; s_ = 3\n",
    "        j2k = list(range(6, 6+self.N-1))  # other agents\n",
    "        b2d = list(range(6+self.N-1, 6+self.N-1 + self.N))  # all actions\n",
    "        e2f = list(range(5+2*self.N, 5+2*self.N + self.N-1))  # all other acts\n",
    "\n",
    "        sumsis = [[j2k[l], s, e2f[l]] for l in range(self.N-1)]  # sum inds\n",
    "        otherX = list(it.chain(*zip((self.N-1)*[Xisa], sumsis)))\n",
    "\n",
    "        NextQis = jnp.einsum(Qisa, [i, s_, a], Xisa, [i, s_, a], [i, s_])\n",
    "                    \n",
    "        args = [self.Omega, [i]+j2k+[a]+b2d+e2f] + otherX +\\\n",
    "            [self.T, [s]+b2d+[s_], NextQis, [i, s_], [i, s, a]]                                            \n",
    "        return jnp.einsum(*args, optimize=self.opti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c81cde-43c1-4dbb-9d33-befc03dcb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(stratSARSA.RPEisa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e5300-f53f-4f90-8e8a-4571686c9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(stratSARSA.NextQisa)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266132d0-22fd-4cc7-9836-b74f25ac20e6",
   "metadata": {},
   "source": [
    "Note, that although `stratSARSA.NextQisa` is computed differently than `stratAC.NextVisa`, they give actually identical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3687e0-44ef-4e43-925a-3d272af3f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = 100 * np.random.rand()\n",
    "\n",
    "maeAC = stratAC(env=env, learning_rates=0.1, discount_factors=0.9, choice_intensities=ci)\n",
    "maeSARSA = stratSARSA(env=env, learning_rates=0.1, discount_factors=0.9, choice_intensities=ci)\n",
    "\n",
    "X = maeAC.random_softmax_strategy()\n",
    "\n",
    "assert np.allclose(maeAC.NextVisa(X) - maeSARSA.NextQisa(X), 0, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c9e9e-c070-40fc-bc7c-a102b560b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

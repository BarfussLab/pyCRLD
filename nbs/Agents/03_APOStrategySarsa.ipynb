{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e50524-4deb-48ca-8390-41e4b7103394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Agents/APOStrategySarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957fc580-aef6-401f-90b0-c83db79ccb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from fastcore.utils import *\n",
    "\n",
    "from pyCRLD.Agents.StrategyBase import strategybase\n",
    "from pyCRLD.Utils.Helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "900e4aad-9e61-4c4c-a547-1540f930536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export   \n",
    "class stratSARSA(aPObase, strategybase):\n",
    "    \"\"\"\n",
    "    Class for CRLD-SARSA agents in strategy space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, learning_rates, discount_factors,\n",
    "                 choice_intensities=1, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : environment object\n",
    "        learning_rates : the learning rate(s) for the agents\n",
    "        discount_factors : the discount factor(s) for the agents\n",
    "        choice_intensities : inverse temperature of softmax / exploitation level\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        Tt = env.T; assert np.allclose(Tt.sum(-1), 1)\n",
    "        Rt = env.R\n",
    "        Ot = env.O\n",
    "        super().__init__(Tt, Rt, Ot, discount_factors, **kwargs)\n",
    "        assert np.allclose(env.F, 0), 'PO learning w final state not def.'\n",
    "\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,2))\n",
    "    def RPEisa(self,\n",
    "               Xisa,  # Joint strategy\n",
    "               norm=False # normalize error around actions? \n",
    "               ) -> np.ndarray:  # RP/TD error\n",
    "        \"\"\"\n",
    "        Compute reward-prediction/temporal-difference error for \n",
    "        strategy SARSA dynamics, given joint strategy `Xisa`.\n",
    "        \"\"\"\n",
    "        R = self.Risa(Xisa)\n",
    "        NextQ = self.NextQisa(Xisa, Risa=R)\n",
    "\n",
    "        n = jnp.newaxis\n",
    "        E = self.pre[:,n,n]*R + self.gamma[:,n,n]*NextQ - 1/self.beta[:, n, n] * jnp.log(Xisa)\n",
    "        E *= self.beta[:,n,n]\n",
    "\n",
    "        E = E - E.mean(axis=2, keepdims=True) if norm else E\n",
    "        return E\n",
    "    \n",
    "    @partial(jit, static_argnums=0)\n",
    "    def NextQisa(self,\n",
    "                 Xisa,          # Joint strategy\n",
    "                 Qisa=None,  # Optional state-action values for speed-up\n",
    "                 Risa=None,  # Optional rewards for speed-up\n",
    "                 Vis=None,   # Optional state values for speed-up\n",
    "                 Tisas=None  # Optional transition for speed-up\n",
    "                ) -> jnp.ndarray: # Next values       \n",
    "        \"\"\"\n",
    "        Compute strategy-average next state-action value for agent `i`, current state `s` and action `a`.\n",
    "        \"\"\"\n",
    "        Qisa = self.Qisa(Xisa, Risa=None, Vis=None, Tisas=None)\\\n",
    "            if Qisa is None else Qisa\n",
    "        \n",
    "        i = 0; a = 1; s = 2; s_ = 3\n",
    "        j2k = list(range(6, 6+self.N-1))  # other agents\n",
    "        b2d = list(range(6+self.N-1, 6+self.N-1 + self.N))  # all actions\n",
    "        e2f = list(range(5+2*self.N, 5+2*self.N + self.N-1))  # all other acts\n",
    "\n",
    "        sumsis = [[j2k[l], s, e2f[l]] for l in range(self.N-1)]  # sum inds\n",
    "        otherX = list(it.chain(*zip((self.N-1)*[Xisa], sumsis)))\n",
    "\n",
    "        NextQis = jnp.einsum(Qisa, [i, s_, a], Xisa, [i, s_, a], [i, s_])\n",
    "                    \n",
    "        args = [self.Omega, [i]+j2k+[a]+b2d+e2f] + otherX +\\\n",
    "            [self.T, [s]+b2d+[s_], NextQis, [i, s_], [i, s, a]]                                            \n",
    "        return jnp.einsum(*args, optimize=self.opti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9d050-f30e-40a6-abba-9b5066a645f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

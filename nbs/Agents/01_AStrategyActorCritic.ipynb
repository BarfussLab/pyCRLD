{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adb73f99-6be9-48a9-83ca-2cd045d3a8be",
   "metadata": {},
   "source": [
    "# Strategy Actor-Critic\n",
    "\n",
    "> CRLD actor-critic agents in strategy space  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055fee3-2b02-4710-9ee4-5394621da31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Agents/StrategyActorCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92234412-f8cf-4b25-afc1-1fc4c2257da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5646f-3771-4c7c-b799-c17fe7afc555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fe169-b091-4317-9206-7fc0371e6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from functools import partial\n",
    "\n",
    "from fastcore.utils import *\n",
    "\n",
    "from pyCRLD.Agents.StrategyBase import strategybase\n",
    "from pyCRLD.Utils.Helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2301d-b92d-4b02-9fc1-9b56bf6a97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class stratAC(strategybase):\n",
    "    \"\"\"\n",
    "    Class for CRLD-actor-critic agents in strategy space.\n",
    "    \"\"\"\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,2))\n",
    "    def RPEisa(self,\n",
    "               Xisa,  # Joint strategy\n",
    "               norm=False # normalize error around actions? \n",
    "               ) -> np.ndarray:  # RP/TD error\n",
    "        \"\"\"\n",
    "        Compute reward-prediction/temporal-difference error for \n",
    "        strategy actor-critic dynamics, given joint strategy `Xisa`.\n",
    "        \"\"\"\n",
    "        R = self.Risa(Xisa)\n",
    "        Vis = self.Vis(Xisa, Risa=R)\n",
    "        NextV = self.NextVisa(Xisa, Vis=Vis)\n",
    "\n",
    "        n = jnp.newaxis\n",
    "        E = self.pre[:,n,n]*R + self.gamma[:,n,n]*NextV - Vis[:,:,n]\n",
    "        E *= self.beta[:,n,n]\n",
    "\n",
    "        E = E - E.mean(axis=2, keepdims=True) if norm else E\n",
    "        return E\n",
    "\n",
    "    \n",
    "    @partial(jit, static_argnums=0)\n",
    "    def NextVisa(self,\n",
    "                 Xisa,      # Joint strategy\n",
    "                 Vis=None,  # Optional values for speed-up\n",
    "                 Tss=None,  # Optional transition for speed-up\n",
    "                 Ris=None,  # Optional reward for speed-up\n",
    "                 Risa=None  # Optional reward for speed-up\n",
    "                ) -> jnp.ndarray: # Next values\n",
    "        \"\"\"\n",
    "        Compute strategy-average next value for agent `i`, current state `s` and action `a`.\n",
    "        \"\"\"\n",
    "        Vis = self.Vis(Xisa, Ris=Ris, Tss=Tss, Risa=Risa) if Vis is None else Vis\n",
    "        \n",
    "        i = 0; a = 1; s = 2; s_ = 3\n",
    "        j2k = list(range(6, 6+self.N-1))  # other agents\n",
    "        b2d = list(range(6+self.N-1, 6+self.N-1 + self.N))  # all actions\n",
    "        e2f = list(range(5+2*self.N, 5+2*self.N + self.N-1))  # all other acts\n",
    "\n",
    "        sumsis = [[j2k[l], s, e2f[l]] for l in range(self.N-1)]  # sum inds\n",
    "        otherX = list(it.chain(*zip((self.N-1)*[Xisa], sumsis)))\n",
    "                                \n",
    "        args = [self.Omega, [i]+j2k+[a]+b2d+e2f] + otherX +\\\n",
    "            [self.T, [s]+b2d+[s_], Vis, [i, s_], [i, s, a]]\n",
    "        return jnp.einsum(*args, optimize=self.opti)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d879d8dc-fb3a-4051-8f29-e23fd7a3c7fd",
   "metadata": {},
   "source": [
    "Note, `choice_intensities` are not required for actor-critic learning and have no other effect than scaling the `learning_rates`. Hence the default value of `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c81cde-43c1-4dbb-9d33-befc03dcb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(stratAC.RPEisa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e5300-f53f-4f90-8e8a-4571686c9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(stratAC.NextVisa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c9e9e-c070-40fc-bc7c-a102b560b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390e8984-e064-4605-b241-425aceee3f4c",
   "metadata": {},
   "source": [
    "# Strategy Base (part. Obs.)\n",
    "\n",
    ">Base class containing the core methods of CRLD agents learning under partial observability in strategy space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6f499-35fb-463a-8d09-c09d73e6fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Agents/POStrategyBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c558329-3dff-4a2a-aea8-85f0ea2de4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9366e48b-3fe6-477a-bb2b-e20195aac8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99ae26-8c5d-4cd1-ac1a-1fa34264dfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import jax\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from functools import partial\n",
    "\n",
    "from fastcore.utils import *\n",
    "\n",
    "from pyCRLD.Agents.Base import abase\n",
    "from pyCRLD.Agents.POBase import aPObase\n",
    "from pyCRLD.Agents.StrategyBase import strategybase\n",
    "from pyCRLD.Utils.Helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c2539-a242-45bd-a6e5-25ab9a76285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class POstrategybase(aPObase, strategybase):\n",
    "    \"\"\"\n",
    "    Base Class for\n",
    "    deterministic policy-average independent (multi-agent) partially observable\n",
    "    temporal-difference reinforcement learning in policy space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, learning_rates, discount_factors,\n",
    "                 choice_intensities=1, **kwargs):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : environment object\n",
    "        learning_rates : the learning rate(s) for the agents\n",
    "        discount_factors : the discount factor(s) for the agents\n",
    "        choice_intensities : inverse temperature of softmax / exploitation level\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        Tt = env.T; assert np.allclose(Tt.sum(-1), 1)\n",
    "        Rt = env.R\n",
    "        Ot = env.O    \n",
    "        super().__init__(Tt, Rt, Ot, discount_factors, **kwargs)\n",
    "        assert np.allclose(env.F, 0), 'PO learning w final state not def.'\n",
    "\n",
    "        # learning rates\n",
    "        self.alpha = make_variable_vector(learning_rates, self.N)\n",
    "        \n",
    "        # intensity of choice\n",
    "        self.beta = make_variable_vector(choice_intensities, self.N)\n",
    "\n",
    "        self.TDerror = self.RPEioa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c87701-7d16-421b-a7d2-2591cbc3d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def random_softmax_policy(self:POstrategybase):\n",
    "            \"\"\"Softmax policy with random probabilities.\"\"\"\n",
    "            expQ = jnp.exp(np.random.randn(self.N, self.Q, self.M))\n",
    "            return expQ / expQ.sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95bd9f-c9e7-4877-aaf7-683fa120f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def zero_intelligence_policy(self:POstrategybase):\n",
    "            \"\"\"Policy with equal probabilities.\"\"\"\n",
    "            return jnp.ones((self.N, self.Q, self.M)) / float(self.M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60bf47-9412-4ff4-b27f-425edc5ff968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c749e2-fa63-45c5-b5cf-a06f50df95de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53577c24-6d85-432f-9891-342d4a76319b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

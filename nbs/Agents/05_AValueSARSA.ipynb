{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value SARSA\n",
    "\n",
    "> CRLD SARSA agents in value space  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value-based SARSA dynamics have been developed and used in the paper, [Intrinsic fluctuations of reinforcement learning promote cooperation](https://www.nature.com/articles/s41598-023-27672-7) by Barfuss W, Meylahn J in Sci Rep 13, 1309 (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Agents/ValueSARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCRLD.Agents.ValueSARSA import valSARSA\n",
    "from pyCRLD.Agents.ValueBase import multiagent_epsilongreedy_strategy\n",
    "\n",
    "from pyCRLD.Environments.SocialDilemma import SocialDilemma\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an epsilon-greedy strategy function. This strategy function selects the best action with probability 1-epsilon and a random action with probability epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsgreedy = multiagent_epsilongreedy_strategy(epsilon_greedys=0.01, N=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the environment to be a simple stag-hunt game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SocialDilemma(R=1.0, T=0.75, S=-0.15, P=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use both, the strategy function and the envrionment to create the value-based SARSA multi-agent environment objecte `mae`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mae = valSARSA(env, discount_factors=0.5, learning_rates=0.1, strategy_function=epsgreedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We illustrate the value-based SARSA agents by sampling from 50 random initial values and plot the resulting learning times series in value space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "        Qinit = mae.random_values()\n",
    "        Qtisa, fpr = mae.trajectory(Qinit, Tmax=1000, tolerance=10e-9)\n",
    "\n",
    "        # plot time series\n",
    "        plt.plot(Qtisa[:, 0, 0, 0]- Qtisa[:, 0, 0, 1], \n",
    "                Qtisa[:, 1, 0, 0]- Qtisa[:, 1, 0, 1],\n",
    "                '.-', alpha=0.1, color='blue')\n",
    "        # plot last point\n",
    "        plt.plot(Qtisa[-1, 0, 0, 0]- Qtisa[-1, 0, 0, 1],\n",
    "                Qtisa[-1, 1, 0, 0]- Qtisa[-1, 1, 0, 1],\n",
    "                '.', color='red')\n",
    "\n",
    "# plot quadrants\n",
    "plt.plot([0, 0], [-2, 2], '-', color='gray', lw=0.5)\n",
    "plt.plot([-2, 2], [0, 0], '-', color='gray', lw=0.5)\n",
    "\n",
    "plt.xlim(-1.2, 1.2); plt.ylim(-1.2, 1.2)\n",
    "plt.xlabel(r'action-value difference $Q(coop.) - Q(defect)$ of agent 1'); \n",
    "plt.ylabel(r'$Q(coop.) - Q(defect)$ of agent 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from fastcore.utils import *\n",
    "\n",
    "from pyCRLD.Agents.ValueBase import valuebase\n",
    "from pyCRLD.Utils.Helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export   \n",
    "class valSARSA(valuebase):\n",
    "    \"\"\"\n",
    "    Class for CRLD-SARSA agents in value space.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temporal difference reward-prediction error is calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@partial(jit, static_argnums=(0,2))\n",
    "def RPEisa(self:valSARSA,\n",
    "           Qisa,  # Joint strategy\n",
    "           norm=False # normalize error around actions? \n",
    "           ) -> np.ndarray:  # reward-prediction error\n",
    "    \"\"\"\n",
    "    Compute temporal-difference reward-prediction error for \n",
    "    value SARSA dynamics, given joint state-action values `Qisa`.\n",
    "    \"\"\"\n",
    "    Risa = self.valRisa(Qisa)\n",
    "    NextQisa = self.value_NextQisa(Qisa)\n",
    "    \n",
    "    n = jnp.newaxis\n",
    "    E = self.pre[:,n,n]*Risa + self.gamma[:,n,n]*NextQisa - Qisa\n",
    "    \n",
    "    E = E - E.mean(axis=2, keepdims=True) if norm else E\n",
    "    return E\n",
    "valSARSA.RPEisa = RPEisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@partial(jit, static_argnums=0)\n",
    "def valRisa(self:valSARSA, \n",
    "            Qisa): # Joint state-action values\n",
    "    \"\"\" Average reward Risa, given joint state-action values `Qisa` \"\"\"\n",
    "    Xisa = self.strategy_function.action_probabilities(Qisa)\n",
    "    Risa = self.Risa(Xisa)\n",
    "    return Risa\n",
    "valSARSA.valRisa = valRisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@partial(jit, static_argnums=0)\n",
    "def valNextQisa(self:valSARSA, \n",
    "                Qisa):   \n",
    "    \"\"\"\n",
    "    Compute strategy-average next state-action value for agent `i`, current\n",
    "    state `s` and action `a`, given joint state-action values `Qisa`.\n",
    "    \"\"\"\n",
    "    Xisa = self.strategy_function.action_probabilities(Qisa)\n",
    "    valQisa = self.Qisa(Xisa)  # true state-action values given current Qisa\n",
    "\n",
    "    i = 0  # agent i\n",
    "    a = 1  # its action a\n",
    "    s = 2  # the current state\n",
    "    sprim = 3  # the next state\n",
    "    j2k = list(range(4, 4+self.N-1))  # other agents\n",
    "    b2d = list(range(4+self.N-1, 4+self.N-1 + self.N))  # all actions\n",
    "    e2f = list(range(3+2*self.N, 3+2*self.N + self.N-1))  # all other acts\n",
    "\n",
    "    sumsis = [[j2k[l], s, e2f[l]] for l in range(self.N-1)]  # sum inds\n",
    "    otherX = list(it.chain(*zip((self.N-1)*[Xisa], sumsis)))\n",
    "        \n",
    "    NextQisa = jnp.einsum(valQisa, [i, s, a], Xisa, [i, s, a], [i, s])\n",
    "                \n",
    "    args = [self.Omega, [i]+j2k+[a]+b2d+e2f] + otherX +\\\n",
    "        [self.T, [s]+b2d+[sprim], NextQisa, [i, sprim], [i, s, a]]\n",
    "    return jnp.einsum(*args, optimize=self.opti)\n",
    "valSARSA.value_NextQisa = valNextQisa  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

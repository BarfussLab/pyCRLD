{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da35e275-00e5-48fa-84f2-7f0721804e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Environments/HeterogeneousObservationsEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f44aea3-a46f-4db1-8ec3-0780e679ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de765b1-fae2-4d63-b275-deb7de1ad4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42dfe465-c409-4fcf-b40a-0c27724dd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd103bb1-50f6-469d-a073-617a3dcd0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HeterogeneousObservationsEnv(object):\n",
    "    def __init__(self, observation_type='default', observation_value=None):\n",
    "\n",
    "        # Observation configurations\n",
    "        self.observation_type = observation_type\n",
    "        self.observation_value = observation_value\n",
    "\n",
    "        self.transitions = self.transition_tensor()\n",
    "        self.final_states = np.array(self.generate_final_states())\n",
    "        self.rewards = self.reward_tensor()\n",
    "        self.observations = self.generate_observation_tensor()\n",
    "\n",
    "        self.actions_set = self.actions()\n",
    "        self.states_set = self.states() \n",
    "        self.observation_set = self.generate_observation_set()\n",
    "\n",
    "        self.n_agents = self.rewards.shape[0]\n",
    "        self.n_states = self.transitions.shape[0]\n",
    "        self.n_agent_actions = self.transitions.shape[1]\n",
    "\n",
    "        self.n_possible_observations = self.n_states\n",
    "\n",
    "        # Checks\n",
    "        assert all(dim == self.n_agent_actions for dim in self.rewards.shape[2:-1]), 'Inconsistent number of actions'\n",
    "        assert len(self.actions_set) == self.n_agents and all(len(a) == self.n_agent_actions for a in self.actions_set), 'Inconsistent number of actions'\n",
    "        assert self.transitions.shape[-1] == self.n_states and self.rewards.shape[-1] == self.n_states, 'Inconsistent number of states'\n",
    "        assert self.rewards.shape[1] == self.n_states, 'Inconsistent number of states'\n",
    "        assert len(self.final_states) == self.n_states, 'Inconsistent number of states'\n",
    "        assert len(self.states_set) == self.n_states, 'Inconsistent number of states'\n",
    "        assert np.allclose(self.transitions.sum(-1), 1), 'Transition model probabilities do not sum to 1'\n",
    "        assert observations.shape[0] == self.n_agents, \"Inconsistent number of agents\"\n",
    "        assert observations.shape[1] == self.n_states, \"Inconsistent number of states\"\n",
    "        assert np.allclose(observations.sum(-1), 1), 'Observation model probabilities do not sum to 1'\n",
    "\n",
    "        # Ensure naming compatibility with the rest of PyCRDT\n",
    "        self.R = self.rewards\n",
    "        self.N = self.n_agents\n",
    "        self.F = self.final_states\n",
    "        self.M = self.n_agent_actions\n",
    "        self.Z = self.n_states\n",
    "        self.T = self.transitions\n",
    "        self.O = self.observations\n",
    "        self.Oset = self.observation_set\n",
    "        self.Q = self.n_possible_observations\n",
    "        self.Aset = self.actions_set\n",
    "        self.Sset = self.states_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cc0a4ef-1c5c-424e-87bc-8b32930be2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def id(self:HeterogeneousObservationsEnv):\n",
    "    \"\"\"Returns id string of environment\"\"\"\n",
    "    return f\"{self.__class__.__name__}\"\n",
    "\n",
    "@patch\n",
    "def __str__(self:HeterogeneousObservationsEnv): return self.id()\n",
    "\n",
    "@patch\n",
    "def __repr__(self:HeterogeneousObservationsEnv): return self.id()\n",
    "\n",
    "@patch\n",
    "def transition_tensor(self:HeterogeneousObservationsEnv):\n",
    "    raise NotImplementedError\n",
    "@patch\n",
    "def reward_tensor(self:HeterogeneousObservationsEnv):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@patch\n",
    "def generate_observation_tensor(self:HeterogeneousObservationsEnv):\n",
    "    self.n_possible_observations = self.n_states\n",
    "    # Initialize the observation tensor filled with zeros.\n",
    "    observations_iso = np.zeros((self.n_agents, self.n_states, self.n_possible_observations))\n",
    "\n",
    "    # Check if observation_type and observation_value are lists\n",
    "    if isinstance(self.observation_type, list) and isinstance(self.observation_value, list):\n",
    "        for agent_index, (obs_type, focused_value) in enumerate(zip(self.observation_type, self.observation_value)):\n",
    "            self._apply_observation_configuration(agent_index, obs_type, focused_value, observations_iso)\n",
    "    else:\n",
    "        # Handle the case where observation_type and observation_value are single values\n",
    "        obs_type = self.observation_type\n",
    "        focused_value = self.observation_value if isinstance(self.observation_value, list) else [self.observation_value]\n",
    "        for agent_index in range(self.n_agents):\n",
    "            self._apply_observation_configuration(agent_index, obs_type, focused_value[0], observations_iso)\n",
    "\n",
    "    return observations_iso\n",
    "\n",
    "@patch\n",
    "def _apply_observation_configuration(self:HeterogeneousObservationsEnv, agent_index, obs_type, focused_value, observations_iso):\n",
    "    if obs_type == 'default':\n",
    "        for state in range(self.n_states):\n",
    "            if self.n_possible_observations > 1:\n",
    "                remaining_value = (1 - focused_value) / (self.n_possible_observations - 1)\n",
    "            else:\n",
    "                remaining_value = 0.0\n",
    "            observations_iso[agent_index, state, :] = remaining_value\n",
    "            observations_iso[agent_index, state, state] = focused_value\n",
    "    elif obs_type == 'diagonal_confidence':\n",
    "        for state in range(self.n_states):\n",
    "            observations_iso[agent_index, state, state] = focused_value\n",
    "    elif obs_type == 'fill':\n",
    "        observations_iso[agent_index, :, :] = focused_value\n",
    "\n",
    "@patch\n",
    "def generate_final_states(self:HeterogeneousObservationsEnv):\n",
    "    \"\"\"Default final states: no final states\"\"\"\n",
    "    return np.zeros(self.n_states, dtype=int)\n",
    "\n",
    "@patch\n",
    "def actions(self:HeterogeneousObservationsEnv):\n",
    "    \"\"\"Default action set representations.\"\"\"\n",
    "    return [[str(a) for a in range(self.n_agent_actions)] for _ in range(self.n_agents)]\n",
    "\n",
    "@patch\n",
    "def states(self:HeterogeneousObservationsEnv):\n",
    "    \"\"\"Default state set representation.\"\"\"\n",
    "    return [str(s) for s in range(self.n_states)]\n",
    "\n",
    "@patch\n",
    "def generate_observation_set(self:HeterogeneousObservationsEnv):\n",
    "    \"\"\"Creates observation labels.\"\"\"\n",
    "    # Currently this only generates labels for games with 1 state: '.'\n",
    "    return [[str('.') for o in range(self.n_possible_observations)] for _ in range(self.n_agents)]\n",
    "\n",
    "@patch\n",
    "def step(self:HeterogeneousObservationsEnv, jA:Iterable) -> tuple:\n",
    "    \"\"\"Iterate the environment one step forward.\"\"\"\n",
    "    tps = self.transitions[tuple([self.state]+list(jA))].astype(float)\n",
    "    next_state = np.random.choice(range(len(tps)), p=tps)\n",
    "    rewards = self.rewards[tuple([slice(self.n_agents), self.state]+list(jA)+[next_state])]\n",
    "    self.state = next_state\n",
    "    obs = self.generate_stochastic_observations()\n",
    "    done = self.state in np.where(self.final_states==1)[0]\n",
    "    info = {'state': self.state}\n",
    "    return obs, rewards.astype(float), done, info\n",
    "\n",
    "@patch\n",
    "def generate_stochastic_observations(self:HeterogeneousObservationsEnv) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Produces a set of observations for each agent based on the current state, utilizing the defined observation tensors.\n",
    "    Each tensor represents a different observation model, and this method generates observations according to the probability\n",
    "    distributions specified in those tensors for the current state.\n",
    "    \n",
    "    Returns:\n",
    "        A list of numpy arrays, where each array contains observations for all agents as determined by one of the observation tensors.\n",
    "    \"\"\"\n",
    "    # TODO: I need to ensure the observation arrays sum up to 1\n",
    "\n",
    "    all_agents_observations = []  # Stores observations generated by each observation tensor.\n",
    "    for observation_tensor in self.observations_list:\n",
    "        current_state_observations = np.zeros(self.n_agents, dtype=int)  # Initializes the observation array for this tensor.\n",
    "        for agent_index in range(self.n_agents):\n",
    "            # Retrieves the probability distribution of observations for the current agent and state from the tensor.\n",
    "            observation_probabilities = observation_tensor[agent_index, self.state]\n",
    "            # Generates a random observation based on the probability distribution.\n",
    "            chosen_observation = np.random.choice(range(len(observation_probabilities)), p=observation_probabilities)\n",
    "            current_state_observations[agent_index] = chosen_observation\n",
    "        all_agents_observations.append(current_state_observations)\n",
    "    return all_agents_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88f0ba2-efc5-4c18-9bc9-0d9c8a253f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

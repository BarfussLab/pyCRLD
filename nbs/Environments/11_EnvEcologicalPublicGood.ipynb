{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bb6e51f-dfd0-44fa-a702-96a08301e3e7",
   "metadata": {},
   "source": [
    "# Ecological Public Good\n",
    "\n",
    "> Class for ecological public good environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d0a000c",
   "metadata": {},
   "source": [
    "The environment was introduced in \n",
    "\n",
    "> Barfuss, W., Donges, J. F., Vasconcelos, V. V., Kurths, J., & Levin, S. A. (2020). Caring for the future can turn tragedy into comedy for long-term collective action under risk of collapse. Proceedings of the National Academy of Sciences, 117(23), 12915-12922."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f03146-1f02-4bd7-bc5a-9774aafca256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Environments/EcologicalPublicGood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70985a-597d-4465-8383-40d7a84503fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cbff78-3063-4406-88aa-d03ac35f0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ee25630-40b9-4a5b-8923-fbfbd82b284c",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9833135-dddc-4f93-a4d2-f778101fda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCRLD.Environments.EcologicalPublicGood import EcologicalPublicGood\n",
    "from pyCRLD.Agents.StrategyActorCritic import stratAC\n",
    "from pyCRLD.Utils import FlowPlot as fp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd5f83-3340-4c5a-97c9-25612bcf7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EcologicalPublicGood(N=2, f=1.2, c=5, m=-4, qc=0.2, qr=0.1, degraded_choice=True)\n",
    "env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "287b343f-a76e-46b3-81c6-49f2de1644e3",
   "metadata": {},
   "source": [
    "In the prosperous state, the rewards are a tragedy Prisoners' Dilemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a965430-abf9-497c-8fc9-d3096d9c92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.R[0,1,:,:,1], env.R[1,1,:,:,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3564aee1-1e88-4f36-b188-502fa5e04827",
   "metadata": {},
   "source": [
    "Yet, because of the possible collapse and the agents' future outlook, the overall regime is one of coordination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b170d5b-ff4e-4acb-bc92-4286371e626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init enviornment and MultiAgentEnvironment-interface\n",
    "mae = stratAC(env=env, learning_rates=0.1, discount_factors=0.9)\n",
    "\n",
    "x = ([0], [0,1], [0])  # Plotting on the x-axis the [0]'s agents probability in both states [0,1] to cooprate [0]\n",
    "y = ([1], [0,1], [0])  # Plotting on the y-axis the [1]'s agents probability in both states [0,1] to cooprate [0]\n",
    "ax = fp.plot_strategy_flow(mae, x, y, flowarrow_points = np.linspace(0.01 ,0.99, 9), NrRandom=16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8230489-42e9-4e26-9578-e00610209b1c",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f510399-524c-4756-bfc9-d791de43ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pyCRLD.Environments.Base import ebase\n",
    "from pyCRLD.Utils.Helpers import make_variable_vector\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "\n",
    "from typing import Iterable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446dc68-1fb3-4e54-8425-65da84ebfcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EcologicalPublicGood(ebase):\n",
    "    \"\"\"\n",
    "    Ecological Public Good Environment.\n",
    "    \"\"\" \n",
    "\n",
    "    def __init__(self,\n",
    "                 N:int,  # number of agents\n",
    "                 f:Union[float, Iterable],  # public goods synergy factor\n",
    "                 c:Union[float, Iterable],  # cost of cooperation\n",
    "                 m:Union[float, Iterable], # collapse impact\n",
    "                 qc:Union[float, Iterable], # collapse leverage/timescale\n",
    "                 qr:Union[float, Iterable], # recovery leverage/timescale\n",
    "                 degraded_choice=False):  # whether agents have a choice at the degraded state\n",
    "        self.N = N\n",
    "        self.M = 2\n",
    "        self.Z = 2\n",
    "\n",
    "        self.f = make_variable_vector(f, N)\n",
    "        self.c = make_variable_vector(c, N)\n",
    "        self.m = make_variable_vector(m, N)\n",
    "        self.qc = make_variable_vector(qc, N)\n",
    "        self.qr = make_variable_vector(qr, N)\n",
    "        self.degraded_choice = degraded_choice\n",
    "        \n",
    "        self.Aset = self.actions()  # to have them available for the creation\n",
    "        self.Sset = self.states()   # of the Transition and Reward Tensors\n",
    "\n",
    "        self.state = 1 # inital state\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf48ea1e-aa4c-4cba-a181-9675fa413b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def actions(self:EcologicalPublicGood):\n",
    "    \"\"\"The action sets\"\"\"\n",
    "    return [['c', 'd'] for _ in range(self.N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f4575-0816-4eab-8305-491192b48b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def states(self:EcologicalPublicGood):\n",
    "    \"\"\"The states set\"\"\"\n",
    "    return ['g', 'p'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ce783-63e5-402d-bb92-f42fea047254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def TransitionTensor(self:EcologicalPublicGood):\n",
    "    \"\"\"Get the Transition Tensor.\"\"\"\n",
    "    dim = np.concatenate(([self.Z],\n",
    "                          [self.M for _ in range(self.N)],\n",
    "                          [self.Z]))\n",
    "    Tsas = np.ones(dim) * (-1)\n",
    "\n",
    "    for index, _ in np.ndenumerate(Tsas):\n",
    "        Tsas[index] = self._transition_probability(index[0],\n",
    "                                                   index[1:-1],\n",
    "                                                   index[-1])\n",
    "    return Tsas\n",
    "\n",
    "@patch\n",
    "def _transition_probability(self:EcologicalPublicGood,\n",
    "                            s:int, # the state index\n",
    "                            jA:Iterable, # indices for joint actions\n",
    "                            s_:int  # the next-state index\n",
    "                           ) -> float:  # transition probability \n",
    "    \"\"\"\n",
    "    Returns the transition probability for current state `s`, joint action `jA`, and next state `s_`.\n",
    "    \"\"\"\n",
    "    transitionprob = 0\n",
    "\n",
    "    if self.Sset[s] == 'p': # if we are in the prosperous state\n",
    "        # determine the defectors\n",
    "        defcs = np.where(np.array([act[jA[j]] for j, act\n",
    "                                   in enumerate(self.Aset)]) == 'd')[0]            \n",
    "        # and add up collapse leverages (= per actor collapse probabilites)\n",
    "        transitionprob = np.sum([self.qc[i] for i in defcs]) /self.N\n",
    "\n",
    "        if self.Sset[s_] == 'g': # if we collapsed\n",
    "            return transitionprob  # that is our transition probability\n",
    "        else:  # if we didn't collapse\n",
    "            return 1-transitionprob  # it is the \"inverse\" proability\n",
    "\n",
    "    else:  # if we are in the degraded state\n",
    "        if self.degraded_choice:  # and if the agents' choice matter\n",
    "            # determine cooperator\n",
    "            coops = np.where(np.array([act[jA[j]] for j, act in enumerate(self.Aset)]) == 'c')[0]\n",
    "            # and add up recovery leverages (= per actor recovery probabilites)\n",
    "            transitionprob = np.sum([self.qr[i] for i in coops]) /self.N\n",
    "\n",
    "        else:  # if the agents do not have a choice\n",
    "            transitionprob = self.qr.mean()  # take the average collapse leverage\n",
    "\n",
    "        if self.Sset[s_] == 'p':  # if we recovered\n",
    "            return transitionprob  # that is our transition probability\n",
    "        else:  # if we didnt' recovery\n",
    "            return 1-transitionprob  # it is the \"inverse\" probability     \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8abd64f8-efc7-40f0-88c1-2dc3d8fd4829",
   "metadata": {},
   "source": [
    "The `TransitionTensor` is obtained with the help of the `_transition_probability` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81172fb2-c7f0-4449-a2f5-922c3f19fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EcologicalPublicGood._transition_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca66c71-1ab3-4514-ab1b-15984e6bb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def RewardTensor(self:EcologicalPublicGood):\n",
    "    \"\"\"Get the Reward Tensor R[i,s,a1,...,aN,s'].\"\"\"\n",
    "    dim = np.concatenate(([self.N],\n",
    "                          [self.Z],\n",
    "                          [self.M for _ in range(self.N)],\n",
    "                          [self.Z]))\n",
    "    Risas = np.zeros(dim)\n",
    "\n",
    "    for index, _ in np.ndenumerate(Risas):\n",
    "        Risas[index] = self._reward(index[0], index[1], index[2:-1],\n",
    "                                    index[-1])\n",
    "    return Risas\n",
    "\n",
    "@patch\n",
    "def _reward(self:EcologicalPublicGood,\n",
    "            i:int, # the agent index\n",
    "            s:int, # the state index\n",
    "            jA:Iterable, # indices for joint actions\n",
    "            s_:int  # the next-state index\n",
    "            ) -> float:  # reward value\n",
    "    \"\"\"\n",
    "    Returns the reward value for agent `i` in current state `s`, under joint action `jA`, when transitioning to next state `s_`.\n",
    "    \"\"\"\n",
    "    if self.Sset[s] == 'g' or self.Sset[s_] == 'g':  # if either current or next state is degraded\n",
    "        return self.m[i]  # the agents receive the collapse impact\n",
    "\n",
    "    else:  # if current and next state are prosperous\n",
    "        # determine cooperators\n",
    "        coops = np.where(np.array([act[jA[j]] for j, act in enumerate(self.Aset)]) == 'c')[0]\n",
    "        # and sum up reward contributions\n",
    "        reward = np.sum([self.f[i] * self.c[i] for i in coops])/self.N \n",
    "\n",
    "        if self.Aset[i][jA[i]] == 'c':  # if focal player i is a cooperator\n",
    "            reward -= self.c[i]  # subtract the cost of cooperation\n",
    "        return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "205db98e-63ef-4143-b89c-4e1479270de1",
   "metadata": {},
   "source": [
    "The `RewardTensor` is obtained with the help of the `_reward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7647a26-3091-4cfb-81f6-541c5aae0f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(EcologicalPublicGood._reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c7eb5-c6c4-43cb-9408-8278d319051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def id(self:EcologicalPublicGood):\n",
    "    \"\"\"\n",
    "    Returns id string of environment\n",
    "    \"\"\"\n",
    "    # Default\n",
    "    f = self.f if len(np.unique(self.f))>1 else self.f[0]\n",
    "    c = self.c if len(np.unique(self.c))>1 else self.c[0]\n",
    "    m = self.m if len(np.unique(self.m))>1 else self.m[0]\n",
    "    qc = self.qc if len(np.unique(self.qc))>1 else self.qc[0]\n",
    "    qr = self.qr if len(np.unique(self.qr))>1 else self.qr[0]\n",
    "\n",
    "    id = f\"{self.__class__.__name__}_\"+\\\n",
    "        f\"{self.N}_{str(f)}_{str(c)}_{str(m)}_{str(qc)}_{str(qr)}\"\n",
    "    if not self.degraded_choice:\n",
    "        id += \"_NoDegChoi\"\n",
    "    else:\n",
    "        id += \"_DegChoi\"\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8993c16-00c7-42e7-9950-148ab7e922d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c918c6",
   "metadata": {},
   "source": [
    "# Risk Reward Dilemma\n",
    "> This class models a two-state social dilemma where a single agent chooses between `risky` or `cautious` actions. The actions taken by the agent determine the probability of transitioning between `degraded` and `prosporus` states . In each state, the agent receives different rewards, reflecting the consequences of its chosen action!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Environments/RiskReward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62cc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ac78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1742f2d",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa152311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pyCRLD.Environments.Base import ebase\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01451246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RiskReward(ebase):\n",
    "    \"\"\"\n",
    "    An MDP model for decision-making under uncertainty with two states \n",
    "    (prosperous and degraded) and two actions (cautious and risky).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pc:float, pr:float, rs:float, rr:float, rd:float):\n",
    "        self.pc = pc  # Collapse probability when risky in prosperous\n",
    "        self.pr = pr  # Recovery probability when cautious in degraded\n",
    "        self.rs = rs  # Reward for staying prosperous and cautious\n",
    "        self.rr = rr  # Reward for staying prosperous but risky\n",
    "        self.rd = rd  # Reward when in degraded state\n",
    "        \n",
    "        self.N = 1  # Number of agents\n",
    "        self.M = 2  # Number of actions\n",
    "        self.Z = 2  # Number of states\n",
    "        self.state = 0  # Start in the prosperous state (index 0)\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def TransitionTensor(self:RiskReward):\n",
    "        \"\"\"\n",
    "        Define the Transition Tensor for the MDP.\n",
    "        \"\"\"\n",
    "        T = np.zeros((self.Z, self.M, self.Z))\n",
    "        T[0, 0, 0] = 1       # Prosperous and cautious stays prosperous\n",
    "        T[0, 1, 0] = 1 - self.pc  # Prosperous and risky may stay\n",
    "        T[0, 1, 1] = self.pc      # Prosperous and risky may collapse\n",
    "        T[1, 0, 0] = self.pr      # Degraded and cautious may recover\n",
    "        T[1, 0, 1] = 1 - self.pr  # Degraded and cautious may stay\n",
    "        T[1, 1, 1] = 1       # Degraded and risky stays degraded\n",
    "        return T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def RewardTensor(self:RiskReward):\n",
    "        \"\"\"\n",
    "        Define the Reward Tensor for the MDP.\n",
    "        \"\"\"\n",
    "        R = np.zeros((self.N, self.Z, self.M, self.Z))\n",
    "        R[0, 0, 0, 0] = self.rs  # Prosperous and cautious\n",
    "        R[0, 0, 1, 0] = self.rr  # Prosperous and risky but stays\n",
    "        R[0, 0, 1, 1] = self.rd  # Prosperous and risky but collapses\n",
    "        R[0, 1, :, :] = self.rd  # Degraded state rewards\n",
    "        return R\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebc7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def actions(self:RiskReward):\n",
    "        \"\"\"\n",
    "        Define the actions available in the MDP.\n",
    "        \"\"\"\n",
    "        return [['cautious', 'risky']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def states(self:RiskReward):\n",
    "        \"\"\"\n",
    "        Define the states of the MDP.\n",
    "        \"\"\"\n",
    "        return ['prosperous', 'degraded']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a30d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def id(self:RiskReward):\n",
    "        \"\"\"\n",
    "        Provide an identifier for the environment.\n",
    "        \"\"\"\n",
    "        return f\"{self.__class__.__name__}_pc{self.pc}_pr{self.pr}_rs{self.rs}_rr{self.rr}_rd{self.rd}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb479f88",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RiskReward(pc=0.3,pr=.1,rs=0.6,rr=0.8,rd=0.001)\n",
    "# pc, pr, rs, rr, rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3ee23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RiskReward_pc0.3_pr0.1_rs0.6_rr0.8_rd0.001'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb08c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1. , 0. ],\n",
       "        [0.7, 0.3]],\n",
       "\n",
       "       [[0.1, 0.9],\n",
       "        [0. , 1. ]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.TransitionTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a5ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.6  , 0.   ],\n",
       "        [0.8  , 0.001]],\n",
       "\n",
       "       [[0.001, 0.001],\n",
       "        [0.001, 0.001]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.RewardTensor()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d642dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cautious', 'risky']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9339bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prosperous', 'degraded']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac38cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "998aa58c-c1c4-41fe-9f34-1a91133c87cd",
   "metadata": {},
   "source": [
    "# History Embedding\n",
    "\n",
    "> Embed an environment into a more complex representation of state/observation-action histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464920e1-f24d-49ba-98f8-466a24876243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Environments/HistoryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccd0b1-be31-4539-8ddd-b71e14980d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf5f53-62cc-4f80-b159-2841d941da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e6aa092-ad45-472c-975d-5918d5086754",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99f35f-d960-411d-868c-4d789c9e6a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: true notest\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyCRLD.Environments.SocialDilemma import SocialDilemma\n",
    "from pyCRLD.Environments.EcologicalPublicGood import EcologicalPublicGood\n",
    "\n",
    "from pyCRLD.Agents.StrategyActorCritic import stratAC\n",
    "from pyCRLD.Utils import FlowPlot as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3870f5c-254b-4988-8f8f-727463703b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCRLD.Environments.HistoryEmbedding import HistoryEmbedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffb6d48-a276-4599-ace1-a0f2ac578525",
   "metadata": {},
   "outputs": [],
   "source": [
    "socdi = SocialDilemma(R=1.0, T=1.2, S=-0.5, P=0.0)\n",
    "ecopg = EcologicalPublicGood(N=2, f=1.2, c=5, m=-5, qc=0.2, qr=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4941171-a90c-4bfc-b8cb-51c425c1e660",
   "metadata": {},
   "source": [
    "### Memory-one Prisoner's Dilemma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "587d2c4f-dc56-40ae-ba68-9409c5efbba1",
   "metadata": {},
   "source": [
    "With *history embedding*, we can wrap the standard normal form social dilemma envrionment into one, where the agents condition their action on the actions of the last rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc87537-8fb7-4bb5-a7ad-471c2377c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memo1pd = HistoryEmbedded(socdi, h=(1,1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9cca7dc-70f2-4b50-939f-936f3299b3d0",
   "metadata": {},
   "source": [
    "which has effectively a state set with the four elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd5440-13b2-427d-939c-168044dff2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memo1pd.Sset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b950da2-329e-4db9-8ae3-4ff627dbc8bc",
   "metadata": {},
   "source": [
    "As you can see in the flow plots, this opens the possiblity for cooperation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5867d-19a4-4776-a8f2-d28b3f06db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae1 = stratAC(env=memo1pd, learning_rates=0.1, discount_factors=0.9)\n",
    "x = ([0], [0,1,2,3], [0])\n",
    "y = ([1], [0,1,2,3], [0])\n",
    "ax = fp.plot_strategy_flow(mae1, x, y, flowarrow_points=np.linspace(0.01 ,0.99, 9), NrRandom=32,\n",
    "                           conds=mae1.env.Sset)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "552b14d9-a5b4-4e63-a4e7-9a872e94f777",
   "metadata": {},
   "source": [
    "In contrast to the case where agents do not react to the actions of the past round. Here, the only strategy the agents learn is defection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9391d-1af8-43e2-b30e-924b27cfc68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae0 = stratAC(env=socdi, learning_rates=0.1, discount_factors=0.9)\n",
    "x = ([0], [0], [0])\n",
    "y = ([1], [0], [0])\n",
    "ax = fp.plot_strategy_flow(mae0, x, y, flowarrow_points=np.linspace(0.01 ,0.99, 9), NrRandom=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66e02aaa-010a-4d8d-be76-7c7ad060dbb9",
   "metadata": {},
   "source": [
    "What is the effect of having longer action histories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083c124-0920-40b9-9fbb-ab9077a723ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlen = 2\n",
    "memoXpd = HistoryEmbedded(socdi, h=(1, hlen, hlen))\n",
    "print( len(memoXpd.Sset) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a6a7ce-4bdd-43a8-a03f-bc89e2baa237",
   "metadata": {},
   "outputs": [],
   "source": [
    "maeX = stratAC(env=memoXpd, learning_rates=0.1, discount_factors=0.9)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,9))\n",
    "faps = np.linspace(0.01 ,0.99, 13)\n",
    "x = ([0], [0], [0])\n",
    "y = ([1], [0], [0])\n",
    "fp.plot_strategy_flow(mae1, x, y, flowarrow_points=faps, NrRandom=32, cmap=\"Blues\", axes=[ax])\n",
    "fp.plot_strategy_flow(maeX, x, y, flowarrow_points=faps, NrRandom=32, cmap=\"Reds\", axes=[ax]);\n",
    "ax.set_xlabel(\"Agent 0's cooperation probability\")\n",
    "ax.set_ylabel(\"Agent 1's cooperation probability\")\n",
    "ax.set_title(\"Full cooperation in past rounds\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a33548f3-721d-4513-aa47-860185b3783d",
   "metadata": {},
   "source": [
    "The longer action histories give additional force to mutual cooperation when the agents have cooperated in the past rounds and are close to cooperation. This suggests the hypothesis that longer action histories are beneficial for cooperation to be learned. However, more simulation would be needed to answer this question."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79419306-9117-4b5f-b43f-22bdb61b6226",
   "metadata": {},
   "source": [
    "### Memory-one Ecological Public Good"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de529937-94f4-4581-9691-b27b2b6217ab",
   "metadata": {},
   "source": [
    "What is the effect of condition actions also on the past actions in the ecological public goods envrionment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e02fc5-f606-481c-90a9-29d2604b2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecopg1 = HistoryEmbedded(ecopg, h=(1,1,1))\n",
    "ecopg1.Sset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "188e882c-c4ab-4b91-9fbd-244a2ca21f9e",
   "metadata": {},
   "source": [
    "Visualizing the flow of learning in the prosperous state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629eb1f-d880-4d13-8310-0baeac8d4990",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae1 = stratAC(env=ecopg1, learning_rates=0.1, discount_factors=0.9)\n",
    "x = ([0], [1,3,5,7], [0])\n",
    "y = ([1], [1,3,5,7], [0])\n",
    "ax = fp.plot_strategy_flow(mae1, x, y, flowarrow_points=np.linspace(0.01 ,0.99, 9), NrRandom=32,\n",
    "                           conds=np.array(mae1.env.Sset)[[1,3,5,7]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b13d63c4-695a-411a-80aa-eae1e76dd93f",
   "metadata": {},
   "source": [
    "This flow has similarites to the flow of the memory-1 Prisoner's Dilemma above, yet with more tendency toward cooperation. This is expected, since the ecological public good without memory-1 has also more tendency towards cooperation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c4dff9c-ac64-4178-a66c-cf5b20666cd5",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504a9ca-776a-408d-93ee-7e3dd07f8271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "\n",
    "from pyCRLD.Environments.Base import ebase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be3903d7-0510-4736-9056-c6557cfb7f2e",
   "metadata": {},
   "source": [
    "### Histories"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7eaf1dc0-db7e-419a-9773-b141e607bf15",
   "metadata": {},
   "source": [
    "A history specification determines which realizations of the past the agents will conditions their actions on.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0993b5a0-7a22-40f9-998f-b4c53b6febb3",
   "metadata": {},
   "source": [
    "A history specification `h` is an iterable of length 1+`N`. The first value indicates how many time steps of the state observation the agents will use to conditions their actions on. The remaining values indicate how many actions of each agent are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf1f66-88b2-42b9-8486-3f77b02ad45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_all_histories(env, # An environment\n",
    "                       h,  # A history specification\n",
    "                       attr='Z'): #\n",
    "    assert len(h) == env.N+1\n",
    "    assert np.all(np.array(h)>=0)\n",
    "\n",
    "    hiter = []  # history iterables \n",
    "    # go through the maximum history length\n",
    "    for l in reversed(range(max(h))):\n",
    "        # first: actions\n",
    "        # go through all agents\n",
    "        for n in range(env.N):\n",
    "            # if \n",
    "            if l<h[1+n]:\n",
    "                hiter.append(list(range(env.M)))\n",
    "            else:\n",
    "                hiter.append('.')\n",
    "\n",
    "        # second: state\n",
    "        # if specified hist-length is larger than current length\n",
    "        if h[0] > l:\n",
    "            # append hiter with range of states\n",
    "            hiter.append(list(range(getattr(env, attr))))\n",
    "        else:\n",
    "            # if not: append dummy sign\n",
    "            hiter.append('.')\n",
    "\n",
    "    hists = list(it.product(*hiter)) \n",
    "    return hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2c867-4997-4e1d-a5b6-caf9128ca784",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(_get_all_histories)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "791497ba-2413-4946-8d5f-aadd9087a848",
   "metadata": {},
   "source": [
    "The *default* history specification is `h=(1,0,0)`. Each agent observes information about the previous state, but none about the other agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda0ee0-69b2-465c-a433-a67868c38d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_all_histories(socdi, h=(1,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ab378-e4e7-4d57-85bc-08213571027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_all_histories(ecopg, h=(1,0,0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b95245cb-e27b-44f4-a1e9-5fc38de097a0",
   "metadata": {},
   "source": [
    "Each element of these lists is one *history*. The `'.'` indicates a dummy value for the non-observable actions. As you can see, here, the actions come before the state information - in contrast to the history specification `h`. You can think of time traveling from left to right in each history. First the agents choose their joint action, than they observe some state information, after which they choose another joint action. And so on, and so forth. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ba7287c-75bc-4b3e-b265-f8feca16ee3c",
   "metadata": {},
   "source": [
    "For example, the often used *memory-one* social dilemmas can be obtained by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aba61c-3b0a-4ddf-b44c-fff9be46be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_all_histories(socdi, h=(0,1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e46baf3-00ce-4c7e-a743-331b9d98f6f1",
   "metadata": {},
   "source": [
    "Here, the information about the environment is discarded, indicated by the `'.'`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51121c53-d333-49f1-861f-ccc71a8cf083",
   "metadata": {},
   "source": [
    "But the action-history lengths need not be identical,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a9db4-bcdf-43ee-a091-6bab17e00198",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_all_histories(socdi, h=(0,1,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e67d8c12-0641-42f6-b566-ca246d8d521f",
   "metadata": {},
   "source": [
    "Here, each history contains six elements, since it spans two time steps, and each time step is represented by one element for each agent's action plus one element for the environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6891968-e7a3-4864-b920-f4b8b1c87ba8",
   "metadata": {},
   "source": [
    "Of course, histories can be obtained for any environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3a7b1-b93c-4ff0-8f63-6f904c71f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_all_histories(ecopg, h=(2,1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb0dadab-9fe4-46bd-8d0a-f62e2ec09e4c",
   "metadata": {},
   "source": [
    "With `_get_all_histories` we simply iterate through all state and action indicies. However, we are not checking whether a history is actually possible given the transition probabilities of the envrionment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce900eb6-8bea-40f3-807a-6baad7b33812",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _hist_contains_NotPossibleTrans(env, # An environment \n",
    "                                    hist:Iterable  # A history\n",
    "                                   ) -> bool:  # History impossible?\n",
    "    \"\"\"\n",
    "    Checks whether the history contains transitions which are not possible \n",
    "    with the environment's transition probabilities. \n",
    "    \"\"\"\n",
    "    assert len(hist)%(env.N+1) == 0\n",
    "    maxh = int(len(hist) / (env.N+1)) # max history length\n",
    "\n",
    "    contains = False\n",
    "    # go through history from past to present\n",
    "    s='.'\n",
    "    for step in range(0, maxh):\n",
    "        jA = hist[step*(env.N+1):step*(env.N+1)+env.N]\n",
    "        s_ = hist[step*(env.N+1)+env.N]\n",
    "\n",
    "        # construcing index for transition tensor\n",
    "        ix = [s] if s!='.' else [slice(env.Z)]\n",
    "        ix+= [jA[n] if jA[n]!='.' else slice(env.M) for n in range(env.N)]\n",
    "        ix+= [s_] if s_!='.' else [slice(env.Z)]\n",
    "\n",
    "        # check wheter there is possibility for current s,jA,s' tripple\n",
    "        probability = np.sum(env.T[tuple(ix)])\n",
    "\n",
    "        if probability==0:\n",
    "            contains = True\n",
    "            break\n",
    "        else:\n",
    "            # set new state s to s_\n",
    "            s = s_\n",
    "    return contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f7e17-d7e5-48c8-b38b-1612ed093232",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(_hist_contains_NotPossibleTrans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e856ed2-780c-478f-b445-5d6b2d0e0bb9",
   "metadata": {},
   "source": [
    "For example, in the prosperous state `1` of the ecological public good, when both agents choose the cooperative action `0`, there is no chance to leave the propserous state and enter the degraded state `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9ea80-b05d-44d6-bd4e-cd13eedc5525",
   "metadata": {},
   "outputs": [],
   "source": [
    "_hist_contains_NotPossibleTrans(ecopg, hist=('.', '.', 1, 0, 0, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a89baa2e-2633-4f18-822a-273788513293",
   "metadata": {},
   "source": [
    "Thus, any history that contains this transition is not needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18d7930d-a42d-4709-9569-686ae13696d2",
   "metadata": {},
   "source": [
    "Yet, if only one agent chooses the defective action `0`, a transition to the degraded state becomes possible and corresponding histories cannot be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72ab7b-5d3a-45e1-b3cf-228e7ddf5ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_hist_contains_NotPossibleTrans(ecopg, hist=('.', '.', 1, 1, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ccb4f7-b2e0-4c77-bd1f-67cc074575a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_hist_contains_NotPossibleTrans(ecopg, hist=('.', '.', 1, 0, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf8f98-f42e-4d97-8737-1352884972ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def StateActHistsIx(env, h):\n",
    "    \"\"\"\n",
    "    Returns all state-action histories (in indices) of `env`.\n",
    "\n",
    "    `h` specifies the type of history. \n",
    "    `h` must be an iterable of length 1+N (where N = Nr. of Agents)\n",
    "    The first element of `h` specifies the length of the state-history\n",
    "    Subsequent elements specify the length of the respective action-history\n",
    "    \"\"\"\n",
    "    \n",
    "    # get all hists\n",
    "    Hists = _get_all_histories(env, h)\n",
    "\n",
    "    # Remove squences that are not possible\n",
    "    PossibleHists = Hists.copy()\n",
    "    for hist in Hists:\n",
    "        if _hist_contains_NotPossibleTrans(env, hist):\n",
    "            PossibleHists.remove(hist)\n",
    "    return PossibleHists\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01e01236-87c4-4545-ba03-3772d5b46a61",
   "metadata": {},
   "source": [
    "For example, the *memory-one* social dilemmas is obtained by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34d020-641d-43d5-ba99-fab9d36e964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "StateActHistsIx(socdi, h=(0,1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8846b714-1498-4d74-a5df-7461645f0f98",
   "metadata": {},
   "source": [
    "which is identical to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08fa47d-9060-41c1-a960-b2974f360e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_get_all_histories(socdi, h=(0,1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a72f3db-3b9c-4446-82df-897030eaad02",
   "metadata": {},
   "source": [
    "since all histories are actually possible in the environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e3cd47e-fd0f-4060-bea1-1f811d64d499",
   "metadata": {},
   "source": [
    "However, in our ecological public good example, this is not the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3ccc0-377f-4986-98d7-d338802be92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(StateActHistsIx(ecopg, h=(2,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2c7e0-34e4-4a1d-959e-ac17a7d0186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(_get_all_histories(ecopg, h=(2,1,1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ca13ba6-22f9-4b04-8b2c-af428a42e5c4",
   "metadata": {},
   "source": [
    "Depending on the environment, filtering out impossible histories can lead to a significant performance boost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a19d023-cf5f-4bae-ae71-76f7175069da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def hSset(env, # An environment \n",
    "          h):  # A history specificaiton\n",
    "    '''\n",
    "    String representation of the histories.\n",
    "    '''\n",
    "    hmax = max(h)\n",
    "    \n",
    "    hists = []\n",
    "    for hist in StateActHistsIx(env, h):\n",
    "        hrep = ''\n",
    "        # go through all steps of the history\n",
    "        for step in range(0, hmax):\n",
    "            # first: all actions\n",
    "            for i, n in enumerate(range(step*(env.N+1), step*(env.N+1)+env.N)):\n",
    "                hrep += env.Aset[i][hist[n]] if hist[n]!=\".\" else ''\n",
    "                hrep += ','\n",
    "            # second: append state\n",
    "            hrep += env.Sset[hist[n+1]] if hist[n+1]!=\".\" else ''\n",
    "            hrep += '|'\n",
    "        hists.append(hrep)\n",
    "    \n",
    "    return hists"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76aa123a-f2ee-43be-8d6b-1380adb5f3e0",
   "metadata": {},
   "source": [
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9052ef-127e-4ac9-be5e-708a05248881",
   "metadata": {},
   "outputs": [],
   "source": [
    "hSset(socdi, h=(1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d1a94-3038-496c-928d-85e620fd0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "hSset(ecopg, h=(2,1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94464960-b3bd-45c2-a730-db8237e3053b",
   "metadata": {},
   "source": [
    "### Transitions tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df92ca-b569-4d4d-a6c5-586a43c82a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def histSjA_TransitionTensor(env, h):\n",
    "    \"\"\"\n",
    "    Returns Transition Tensor of `env` with state-action history specification `h`.\n",
    "     \n",
    "    `h` must be an iterable of length 1+N (where N = Nr. of Agents)\n",
    "    The first element of `h` specifies the length of the state-history\n",
    "    Subsequent elements specify the length of the respective action-history\n",
    "    \"\"\"\n",
    "    hmax = max(h)\n",
    "\n",
    "    def _transition_possible(hist, hist_):\n",
    "        hi=hist[env.N+1:]; \n",
    "        hi_=hist_[:-(env.N+1)]\n",
    "        possible = []\n",
    "        for k in range((hmax-1)*(env.N+1)):\n",
    "            poss = (hi[k]=='.') or (hi_[k]=='.') or (hi[k]==hi_[k])\n",
    "            possible.append(poss)\n",
    "        return np.all(possible)\n",
    "\n",
    "    Hists = StateActHistsIx(env, h)\n",
    "\n",
    "    Zh = len(Hists)\n",
    "    Th_dims = list(env.T.shape)\n",
    "    Th_dims[0] = Zh\n",
    "    Th_dims[-1] = Zh\n",
    "    Th = np.zeros(Th_dims)\n",
    "\n",
    "    for i, hist in enumerate(Hists):\n",
    "        for j, hist_ in enumerate(Hists):\n",
    "            # Is the transition possible?\n",
    "            possible = _transition_possible(hist, hist_)  \n",
    "            # Get indices\n",
    "            hix, ix = _transition_ix(env, h, i, hist, j, hist_)\n",
    "        \n",
    "            Th[hix] = possible*env.T[ix]\n",
    "\n",
    "    return Th\n",
    "\n",
    "def _transition_ix(env, h, i, hist, j, hist_):\n",
    "    hmax = max(h)\n",
    "\n",
    "    s = hist[-1]\n",
    "    jA = hist_[(hmax-1)*(env.N+1):(hmax-1)*(env.N+1)+env.N]\n",
    "    s_ = hist_[-1]\n",
    "\n",
    "    # construcing index for transition tensor\n",
    "    jAx = [jA[n] if jA[n]!='.' else slice(env.M) for n in range(env.N)]\n",
    "\n",
    "    # for original tensor\n",
    "    ix = [s] if s!='.' else [slice(env.Z)]\n",
    "    ix += jAx\n",
    "    ix+= [s_] if s_!='.' else [slice(env.Z)]\n",
    "\n",
    "    # for history tensor\n",
    "    hix = [i]+jAx+[j]\n",
    "\n",
    "    return tuple(hix), tuple(ix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b33916f2-d27b-4250-a230-97bd8c3f046a",
   "metadata": {},
   "source": [
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360393fb-19b2-42c1-97d7-7ba9a93eb0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "histSjA_TransitionTensor(socdi, h=(0,1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4787d6-2008-42ca-a0be-1f7e8c9a98de",
   "metadata": {},
   "outputs": [],
   "source": [
    "histSjA_TransitionTensor(ecopg, h=(2,1,1)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcd2e4e3-a705-4ed7-b267-bff628667b04",
   "metadata": {},
   "source": [
    "### Reward tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba48569-d846-418c-8918-693e8596d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def histSjA_RewardTensor(env, h):\n",
    "    \"\"\"\n",
    "    Returns Reward Tensor of `env` with state-action history specification `h`.\n",
    "\n",
    "    `h` must be an iterable of length 1+N (where N = Nr. of Agents)\n",
    "    The first element of `h` specifies the length of the state-history\n",
    "    Subsequent elements specify the length of the respective action-history\n",
    "    \"\"\"\n",
    "    hmax=max(h)  # the maximum history length\n",
    "    l = (env.N+1)*hmax  # length of a single history representation\n",
    "                                    \n",
    "    SAHists = StateActHistsIx(env, h)\n",
    "\n",
    "    # dimension for history reward tensor\n",
    "    Zh = len(SAHists)\n",
    "    dims = list(env.R.shape)\n",
    "    dims[1] = Zh\n",
    "    dims[-1] = Zh\n",
    "\n",
    "    Rh = np.zeros(dims)  # init reward tensor\n",
    "    # go through all pairs of histories\n",
    "    for i, hist in enumerate(SAHists):\n",
    "        for j, hist_ in enumerate(SAHists):\n",
    "            hix, ix = _transition_ix(env, h, i, hist, j, hist_)\n",
    "            hix = tuple([slice(env.N)]+list(hix))\n",
    "            ix = tuple([slice(env.N)]+list(ix))\n",
    "            Rh[hix] = env.R[ix]\n",
    "    \n",
    "    return Rh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00f46542-fd54-45d9-8026-84e78bebd325",
   "metadata": {},
   "source": [
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec2104-b72a-4c4d-a5d1-2cac1e5045e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "histSjA_RewardTensor(socdi, h=(1,1,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165f754e-5e4e-49fd-ba20-928aa23724e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "histSjA_RewardTensor(ecopg, h=(1,1,1)).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "712ffd7a-c774-4913-9a29-0b2e7b824758",
   "metadata": {},
   "source": [
    "### Partial observation and environmental uncertainty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06407f2f-f4e8-4b56-9458-fb2e1ba06b21",
   "metadata": {},
   "source": [
    "Note: These elements are useful for enviroments with state uncertainty or likewise, partial observability. Such are not yet available in this respository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd3e69c-b7cc-49e7-8163-c413131fc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ObsActHistsIx(env, h):\n",
    "    \"\"\"\n",
    "    Returns all obs-action histories of `env`.\n",
    "    \n",
    "    `h` specifies the type of history.\n",
    "    `h` must be an iterable of length 1+N (where N = Nr. of Agents)\n",
    "    The first element of `h` specifies the length of the obs-history\n",
    "    Subsequent elements specify the length of the respective action-history\n",
    "        \n",
    "    Note: Here only partial observability regarding the envrionmental state\n",
    "    applies. Additional partial observability regarding action is treated \n",
    "    seperatly.\n",
    "    \"\"\"\n",
    "    \n",
    "    SAhists = StateActHistsIx(env, h=h)\n",
    "    OAhists = _get_all_histories(env, h=h, attr='Q')\n",
    "\n",
    "    hmax=max(h)  # the maximum history length\n",
    "    l = (env.N+1)*hmax  # length of a single history representation\n",
    "\n",
    "    # Remove squences that are not possible to observe\n",
    "    # for all agents\n",
    "    # check all ohist elements\n",
    "    PossibleOAHists = OAhists.copy()\n",
    "    for oahist in OAhists:\n",
    "        # whether they are observable by agent i\n",
    "        observable = np.zeros(env.N)\n",
    "        # go through all shist elements\n",
    "        for sahist in SAhists:\n",
    "            # check wheter action profile fits\n",
    "            sAs = [list(sahist[k:k+env.N]) for k in range(0, l, env.N+1)]\n",
    "            oAs = [list(oahist[k:k+env.N]) for k in range(0, l, env.N+1)]\n",
    "            if sAs == oAs:\n",
    "                # and then check whether oahist can be observed from sahist\n",
    "                observable += np.prod([env.O[:, sahist[k], oahist[k]]\n",
    "                                    for k in range((env.N+1)*(hmax-h[0])+env.N,\n",
    "                                                   l, env.N+1)], axis=0)\n",
    "        # if oahist can't be observed by any agent\n",
    "        if np.allclose(observable, 0.0):\n",
    "            # remove ohist from ObsHists\n",
    "            PossibleOAHists.remove(oahist)\n",
    "    return PossibleOAHists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f2949-c9dc-4a62-a5ba-e7792044e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsActHistsIx(socdi, h=(1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a50015-d306-4b45-af57-d40607ca4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def hOset(env, h):\n",
    "    hmax = max(h)\n",
    "    \n",
    "    all_hists = []\n",
    "    for agent in range(env.N):\n",
    "        hists = []\n",
    "        for hist in ObsActHistsIx(env, h):\n",
    "            hrep = ''\n",
    "            # go through all steps of the history\n",
    "            for step in range(0, hmax):\n",
    "                # first: all actions\n",
    "                for i, n in enumerate(range(step*(env.N+1), step*(env.N+1)+env.N)):\n",
    "                    hrep += env.Aset[i][hist[n]] if hist[n]!=\".\" else ''\n",
    "                    hrep += ','\n",
    "                # second: append observation\n",
    "                hrep += env.Oset[agent][hist[n+1]] if hist[n+1]!=\".\" else ''\n",
    "                hrep += '|'\n",
    "            hists.append(hrep)\n",
    "        all_hists.append(hists)\n",
    "    \n",
    "    return all_hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89717cc4-4dc6-4c20-a205-5413e66e0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "hOset(socdi, h=(1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386022a-134e-477d-9338-f4ed9ff2c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def histSjA_ObservationTensor(env, h):\n",
    "    \"\"\"\n",
    "    Returns Observation Tensor of `env` with state-action history `h`[iterable]\n",
    "    \"\"\"\n",
    "    hmax=max(h)  # the maximum history length\n",
    "    l = (env.N+1)*hmax  # length of a single history representation\n",
    "\n",
    "    SAhists = StateActHistsIx(env, h=h)\n",
    "    OAhists = ObsActHistsIx(env, h=h)\n",
    "\n",
    "    Qh = len(OAhists)\n",
    "    Zh = len(SAhists)\n",
    "    Oh = np.zeros((env.N, Zh, Qh))\n",
    "\n",
    "    # go through each sh oh pair\n",
    "    for i, sahist in enumerate(SAhists):\n",
    "        for j, oahist in enumerate(OAhists):\n",
    "            # check wheter action profile fits\n",
    "            sAs = [list(sahist[k:k+env.N]) for k in range(0, l, env.N+1)]\n",
    "            oAs = [list(oahist[k:k+env.N]) for k in range(0, l, env.N+1)]\n",
    "            if sAs == oAs:\n",
    "                Oh[:, i, j] = np.prod([env.O[:, sahist[k], oahist[k]]\n",
    "                                for k in range((env.N+1)*(hmax-h[0])+env.N,\n",
    "                                                l, env.N+1)], axis=0)\n",
    "    return Oh           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be9bb1-4c22-4b55-9a19-b3901d291ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "histSjA_ObservationTensor(socdi, h=(1,1,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d65e92d-ea57-4a2a-8168-2e8754ec522d",
   "metadata": {},
   "source": [
    "### Environment wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a2c601-3e78-4e6f-973c-8751ef502dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HistoryEmbedded(ebase):\n",
    "    \"\"\"\n",
    "    Abstract Environment wrapper to embed a given environment into a larger\n",
    "    history space\n",
    "    \n",
    "    `h` must be an iterable of length 1+N (where N=Nr. of Agents)\n",
    "    The first element of `history` specifies the length of the state-history.\n",
    "    Subsequent elements specify the length of the respective action-history\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 env, # An environment\n",
    "                 h):  # History specification\n",
    "        self.baseenv = env\n",
    "        self.h = h\n",
    "        \n",
    "        self.N = self.baseenv.N\n",
    "        self.M = self.baseenv.M\n",
    "        self.Z = len(self.states())\n",
    "        self.Q = len(self.observations())\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "    def actions(self):\n",
    "        return self.baseenv.Aset\n",
    "\n",
    "    def states(self):\n",
    "        return hSset(self.baseenv, self.h)\n",
    "    \n",
    "    def observations(self):\n",
    "        return hOset(self.baseenv, self.h)\n",
    "    \n",
    "    def TransitionTensor(self):\n",
    "        return histSjA_TransitionTensor(self.baseenv, self.h)\n",
    "    \n",
    "    def RewardTensor(self):\n",
    "        return histSjA_RewardTensor(self.baseenv, self.h)\n",
    "    \n",
    "    def ObservationTensor(self):\n",
    "        return histSjA_ObservationTensor(self.baseenv, self.h)\n",
    "    \n",
    "    def id(self):\n",
    "        id = f\"{self.__class__.__name__}{self.baseenv.id()}_h{self.h}\"\n",
    "        return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a4d7f-e60b-475d-9278-abbf2f17a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

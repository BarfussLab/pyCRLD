{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a919fe9-2e65-4c4b-9788-b3f13a52f7a5",
   "metadata": {},
   "source": [
    "# Environment Base\n",
    "\n",
    "> Base class for CRLD environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf301c-02c8-4e75-9152-10b80cd2f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp Environments/Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f31d0-f644-4140-880b-110e11fa7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Imports for the nbdev development environment\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438b984-bf78-4148-b458-f4aec934e903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63371478-3ec1-4807-9608-e9850ee91a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2785d-cc5a-44e9-bea6-23d69e815aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ebase(object):\n",
    "    \"\"\"Base environment. All environments should inherit from this one.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "               \n",
    "        self.T = self.TransitionTensor()\n",
    "        self.F = np.array(self.FinalStates())\n",
    "        self.R = self.RewardTensor()\n",
    "        self.O = self.ObservationTensor()\n",
    "                \n",
    "        self.Aset = self.actions()\n",
    "        self.Sset = self.states() \n",
    "        self.Oset = self.observations()\n",
    "\n",
    "        # CHECKS\n",
    "        R, T, O = self.R, self.T, self.O\n",
    "        \n",
    "        # number of agents\n",
    "        N = R.shape[0]  \n",
    "        assert O.shape[0] == N, \"Inconsistent number of agents\"\n",
    "        assert len(T.shape[1:-1]) == N, \"Inconsistent number of agents\"\n",
    "        assert len(R.shape[2:-1]) == N, \"Inconsistent number of agents\"\n",
    "        \n",
    "        # number of actions for each agent        \n",
    "        M = T.shape[1] \n",
    "        assert np.allclose(T.shape[1:-1], M), 'Inconsistent number of actions'\n",
    "        assert np.allclose(R.shape[2:-1], M), 'Inconsistent number of actions'\n",
    "        assert np.all(list(map(len, self.Aset)) == np.array(M).repeat(N)),\\\n",
    "            'Inconsistent number of actions'\n",
    "            \n",
    "        # number of states\n",
    "        Z = T.shape[0] \n",
    "        assert T.shape[-1] == Z, 'Inconsistent number of states'\n",
    "        assert R.shape[-1] == Z, 'Inconsistent number of states'\n",
    "        assert R.shape[1] == Z, 'Inconsistent number of states'\n",
    "        assert O.shape[1] == Z, 'Inconsistent number of states'\n",
    "        assert len(self.F) == Z, 'Inconsistent number of states'\n",
    "        assert len(self.Sset) == Z, 'Inconsistent number of states'\n",
    "\n",
    "        # number of observations\n",
    "        Q = O.shape[-1]\n",
    "        assert np.all(list(map(len, self.Oset)) == np.array(Q).repeat(N)),\\\n",
    "            'Inconsistent number of observations'\n",
    "        \n",
    "        assert np.allclose(T.sum(-1), 1), 'Transition model wrong'\n",
    "        # TODO: Observations dont have to sum up[ to 1 anymore because we might have observations = \n",
    "        # 0.5 for partial observable agents\n",
    "        # Make sure this is correct. Should Obs be zero? or should they always sum up to 1?\n",
    "        # I don't think that's possible for ?\n",
    "        # assert np.allclose(O.sum(-1), 1), 'Observation model wrong'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d986e0d5-d05e-46a2-b51a-91307c66a410",
   "metadata": {},
   "source": [
    "The `ebase` class `__init__` mostly contains consistency checks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a2cc6ec-73ad-48be-a3d6-73ec3f4fd8cc",
   "metadata": {},
   "source": [
    "## Core methods\n",
    "\n",
    "These need to be implemented by a concrete environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e1b32f0-b34d-4aae-aa43-5c0107d4f6db",
   "metadata": {},
   "source": [
    "The transitions tensor `Tsjas'` gives the probability of the environment to transition to state `s'`, given that it was in state `s` and the agent chose the joint action `ja`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc6da1-4ed5-4168-bb24-ea5f162eb4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def TransitionTensor(self:ebase):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe6250-b171-4cbb-842d-9f035f4a21e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class slf: pass\n",
    "test_fail(ebase.TransitionTensor, args=slf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cce1804-a11a-4ef5-b0d5-9dfe66113dd3",
   "metadata": {},
   "source": [
    "raises `NotImplementedError`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "093a75a4-4a50-46e6-aee1-74b2c7794d2e",
   "metadata": {},
   "source": [
    "The reward tensor `Risjas'` gives the reward agent `i` receives when the environment is in state `s`, all agents choose the join action `ja`, and the environment transitions to state `s'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6896a8c-94ae-4e1f-973a-aa77dd3e1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def RewardTensor(self:ebase):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3990fd2-cb45-46bb-99a2-c8030e43ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class slf: pass\n",
    "test_fail(ebase.RewardTensor, args=slf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf6183eb-7d3a-4384-91a8-0244d99c0a0c",
   "metadata": {},
   "source": [
    "raises `NotImplementedError`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1ae7c64-f042-448b-93e8-f6d8cb87fde9",
   "metadata": {},
   "source": [
    "The following two \"core\" methods are optional. If the concrete environment class does not implement them, they default to the following:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b36660a3-dc97-4496-82b9-f52a5774f9ef",
   "metadata": {},
   "source": [
    "The observation tensor `Oiso` gives the probability that agent `i` observes observation `o` when the environment is in state `s`. The default observation tensor assumes perfect observation and sets the number of observations `Q` to the number of states `Z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38af0fa-65b1-4679-805f-8ec2e459db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def ObservationTensor(self:ebase):\n",
    "    \"\"\"Default observation tensor: perfect observation\"\"\"\n",
    "    self.defaultObsTensUsed = True\n",
    "    self.Q = self.Z\n",
    "    Oiso = np.ones((self.N, self.Z, self.Q))\n",
    "    for i in range(self.N):\n",
    "        Oiso[i, :, :] = np.eye(self.Q)\n",
    "    return Oiso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531568d-f3d3-4cc8-ada1-cbe3ee59e4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0.],\n",
       "        [0., 1.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.]],\n",
       "\n",
       "       [[1., 0.],\n",
       "        [0., 1.]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class slf: Z = 2; N = 3  # dummy self for demonstration only\n",
    "ebase.ObservationTensor(slf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c086b690-d177-442a-911f-87ad8a802729",
   "metadata": {},
   "source": [
    "Final states `Fs` indicate which states of the environment cause the end of an episode. Their meaning and use within CRLD are not fully resolved yet. If an environment does not implement `FinalStates` they default to no final states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625d786-8a9f-4d00-9ad8-0a5f0975f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def FinalStates(self:ebase):\n",
    "    \"\"\"Default final states: no final states\"\"\"\n",
    "    return np.zeros(self.Z, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d3ddff-8e60-449b-b00c-e273af9f8e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class slf: Z = 7 # dummy self for demonstration only\n",
    "ebase.FinalStates(slf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc31a75f-30f8-4e8b-b06e-bc6b4eb4ae10",
   "metadata": {},
   "source": [
    "## Default string representations\n",
    "String representations of actions, states and observations help with interpreting the results of simulation runs. Ideally, an environment class will implement these methods with descriptive values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39c103ac-8920-4a33-9823-451448e798b5",
   "metadata": {},
   "source": [
    "To show these methods here we create a dummy \"self\" of 2 environmental states, containing 3 agents with 4 actions and 5 observations of the environmental states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b2cb9-068c-414b-86e0-c6c820efd260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy self of 2 environmental 2 agents with 3 actions in an environment\n",
    "class slf: Z = 2; N = 3; M=4; Q=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d86f4-57df-4219-8489-0bce6f2b6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def actions(self:ebase):\n",
    "    \"\"\"Default action set representations `act_im`.\"\"\"\n",
    "    return [[str(a) for a in range(self.M)] for _ in range(self.N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f032c1-0ab8-42d3-b597-a9cc245a26cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1', '2', '3'], ['0', '1', '2', '3'], ['0', '1', '2', '3']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebase.actions(slf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8f5ac-82a8-4799-b6a1-a7474313822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def states(self:ebase):\n",
    "    \"\"\"Default state set representation `state_s`.\"\"\"\n",
    "    return [str(s) for s in range(self.Z)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce1bf3-e0ad-4723-be1a-3c1e30c2cfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebase.states(slf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0fb51-4ca6-45c5-a65e-5b9e8a74bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def observations(self:ebase):\n",
    "    \"\"\"Default observation set representations `obs_io`.\"\"\"\n",
    "    if hasattr(self, 'defaultObsTensUsed'):\n",
    "        return [[str(o) for o in self.states()] for _ in range(self.N)]\n",
    "    else:\n",
    "        return [[str(o) for o in range(self.Q)] for _ in range(self.N)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fe2da8-2709-4087-ba66-9b385e5d25ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1', '2', '3', '4'],\n",
       " ['0', '1', '2', '3', '4'],\n",
       " ['0', '1', '2', '3', '4']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ebase.observations(slf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025fc92-aafe-4034-9fe9-51066b24c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def id(self:ebase):\n",
    "    \"\"\"\n",
    "    Returns id string of environment\n",
    "    \"\"\"\n",
    "    # Default\n",
    "    id = f\"{self.__class__.__name__}\"\n",
    "    return id\n",
    "\n",
    "@patch\n",
    "def __str__(self:ebase): return self.id()\n",
    "\n",
    "@patch\n",
    "def __repr__(self:ebase): return self.id()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b878916e-6049-403e-9bad-95025816ccb8",
   "metadata": {},
   "source": [
    "## Interactive use\n",
    "Environments can also be used interactivly, e.g., with iterative learning algorithms. For this purpose we provide the [OpenAI Gym `step` Interface](https://github.com/openai/gym#api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b3e49-f5f0-40af-ae21-b107d601c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def step(self:ebase, \n",
    "         jA:Iterable # joint actions\n",
    "        ) -> tuple:  # (observations_i, rewards_i, done, info)\n",
    "    \"\"\"\n",
    "    Iterate the environment one step forward.\n",
    "    \"\"\"\n",
    "    # choose a next state according to transition tensor T\n",
    "    tps = self.T[tuple([self.state]+list(jA))].astype(float)\n",
    "    next_state = np.random.choice(range(len(tps)), p=tps)\n",
    "\n",
    "    # obtain the current rewards\n",
    "    rewards = self.R[tuple([slice(self.N),self.state]+list(jA)\n",
    "                           +[next_state])]\n",
    "\n",
    "    # advance the state and collect info\n",
    "    self.state = next_state\n",
    "    obs = self.observation()     \n",
    "\n",
    "    # if state is a final state the episode is done\n",
    "    done = self.state in np.where(self.F==1)[0]\n",
    "\n",
    "    # report the true state in the info dict\n",
    "    info = {'state': self.state}\n",
    "\n",
    "    return obs, rewards.astype(float), done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb8e70-910d-4147-846a-0e1782e8e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def observation(self:ebase\n",
    "               ) -> np.ndarray:  # observations_i\n",
    "    \"\"\"\n",
    "    Possibly random observation for each agent from the current state.\n",
    "    \"\"\"\n",
    "    OBS = np.zeros(self.N, dtype=int)\n",
    "    for i in range(self.N):\n",
    "        ops = self.O[i, self.state]\n",
    "        obs = np.random.choice(range(len(ops)), p=ops)\n",
    "        OBS[i] = obs\n",
    "    return OBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d311ce1-4c24-4210-8248-605b09ebcb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

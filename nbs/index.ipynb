{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRLD\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collective Reinforcement Learning Dynamics** in Python\n",
    "\n",
    "is a tool to model the collective dynamics emerging from multi-agent reinforcement learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-agent reinforcement learning (MARL) provides a comprehensive framework for studying the interplay among learning, representation, and decision-making between multiple actors.\n",
    "As a result, it offers an integrating platform to in-silico test hypotheses and build theory on how different cognitive mechanisms affect collective adaptive behavior in complex environments.\n",
    "\n",
    "![](0images/MAEi_core.png)\n",
    "\n",
    "In combination with advances in machine learning, particularly deep learning, modern MARL has produced spectacular successes in high-dimensional environments. \n",
    "However, standard RL simulations have significant disadvantages for modeling the collective behavior emerging from MARL: they are *noisy*, sometimes *hard to explain*, *sample-inefficient*, and *computationally intense*.\n",
    "\n",
    "**CRLD** offers a solution in two ways of idealization.\n",
    "First, CRLD aims to understand the principles behind collective behavior in idealized, low-dimensional environments. Second, CRLD concentrates on the essence of the stochastic and computationally intense reinforcement learning algorithms by deriving their strategy-average, deterministic learning equations. \n",
    "\n",
    "In a nutshell, reinforcement learning agents strive to *improve* the rewards they receive while inter*act*ing with the environment. In each time step, they asses a sample of their current *reward-prediction error* $\\delta$. \n",
    "\n",
    "The key idea of CRLD is to replace the individual sample realizations with its strategy average plus a small error term,\n",
    "\n",
    "$$\n",
    "\\delta \\leftarrow \\bar\\delta + \\epsilon.\n",
    "$$\n",
    "\n",
    "One can interpret these learning dynamics from a cognitive and an engineering perspective. In the limit of a vanishing error term, $\\epsilon \\rightarrow 0$, agents have a perfect model of the current environment (cognitive interpretation) or an infinite replay buffer (engineering interpretation)\n",
    "\n",
    "How to put these ideas into practice?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install the package from github:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install \"git+https://github.com/wbarfuss/pyCRLD.git\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# !pip install -e './..[dev]' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "Second, we create a minimal example of a phase space portrait of the learning dynamics in a classic social dilemma environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyCRLD.Agents.StrategyActorCritic import stratAC\n",
    "from pyCRLD.Environments.SocialDilemma import SocialDilemma\n",
    "from pyCRLD.Utils import FlowPlot as fp\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Init enviornment and MultiAgentEnvironment-interface\n",
    "env = SocialDilemma(R=1.0, T=0.8, S=-0.5, P=0.0)\n",
    "mae = stratAC(env=env, learning_rates=0.1, discount_factors=0.9)\n",
    "\n",
    "# Compute learning trajectory \n",
    "x = mae.random_softmax_strategy()  # from a random inital strategy\n",
    "xtraj, fixedpointreached = mae.trajectory(x)\n",
    "\n",
    "# PLOT\n",
    "fig, axs = plt.subplots(1,2, figsize=(9,4))\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Plot in phase space\n",
    "x = ([0], [0], [0])\n",
    "y = ([1], [0], [0])\n",
    "ax = fp.plot_strategy_flow(mae, x, y, flowarrow_points = np.linspace(0.01 ,0.99, 9), axes=[axs[0]])\n",
    "fp.plot_trajectories([xtraj], x, y, cols=['purple'], axes=ax);\n",
    "ax[0].set_xlabel(\"Agent 0's cooperation probability\")\n",
    "ax[0].set_ylabel(\"Agent 1's cooperation probability\");\n",
    "ax[0].set_title(\"Flowplot\")\n",
    "\n",
    "# Plot in trajectory\n",
    "axs[1].plot(xtraj[:, 0, 0, 0], label=\"Agent 0\", c='red')\n",
    "axs[1].plot(xtraj[:, 1, 0, 0], label=\"Agent 1\", c='blue')\n",
    "axs[1].set_xlabel('Time steps')\n",
    "axs[1].set_ylabel('Cooperation probability')\n",
    "axs[1].legend()\n",
    "axs[1].set_title(\"Trajectory\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see how the learning trajectories on the right result from the flow on the left, which suggests that in this environment, mutual cooperation and mutual defection are viable solutions, depending on the initial cooperation levels of both agents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
